# Structured Reasoning Integration Plan

## Executive Summary

Based on the ChatGPT/Gemini discussion, this plan integrates structured reasoning capabilities into the existing Actor-Critic system:

1. **ArgumentNode** - Logical decomposition with evidence links
2. **WeaknessType** - Fixed vocabulary for logical fallacy detection
3. **Plan-and-Write** - Two-step process for 2000+ word reports
4. **ProcessUpdate** - User-friendly SSE progress messages

**Core Principle**: Enhance existing system with optional features, zero breaking changes.

---

## Integration Approach

### Schema Design: Optional Fields + Feature Flags

**Strategy**: Add new enhanced schemas that inherit from existing schemas, controlled by config flags.

**Why This Works**:
- Existing `AnalystResearchOutput`, `CriticReviewOutput`, `WriterComposeOutput` remain unchanged
- New fields are optional (default `None`) - LLM can fail to generate them without breaking flow
- Feature flags enable gradual rollout and easy rollback
- Pydantic validation ensures type safety

---

## ‚ö†Ô∏è CRITICAL: GeminiÂÑ™ÂåñÂª∫Ë≠∞ - ÂØ¶‰ΩúÂâçÂøÖËÆÄ

Ê†πÊìöGeminiÂ∞çÊú¨Ë®àÁï´ÁöÑÂØ©Êü•Ôºå‰ª•‰∏ã**‰∏âÂÄãÈ≠îÈ¨ºÁ¥∞ÁØÄ**ÂøÖÈ†àÂú®ÂØ¶‰ΩúPhase 1Ââç‰øÆÊ≠£Ôºö

### üî¥ Issue 1: Writer.plan() ÁöÑ Token ÁÆ°ÁêÜÂïèÈ°å

**ÂéüË®àÁï´ÂïèÈ°å**:
```python
# ‚ùå Âè™ÂèñÂâç500Â≠óÂÖÉ
{analyst_draft[:500]}...
```

**È¢®Èö™**: Â¶ÇÊûúAnalystÂÅö‰∫Ü3Ëº™Ê∑±Â∫¶ÊêúÂ∞ãÔºåËçâÁ®øÂèØËÉΩ2000-3000Â≠óÔºåÊà™Êñ∑Âú®500Â≠óÊúÉ‰∏üÂ§±ÂæåÊúüÁôºÁèæÁöÑÈóúÈçµË≥áË®äÔºåÂ∞éËá¥WriterË¶èÂäÉÁöÑÂ§ßÁ∂±„ÄåÊñá‰∏çÂ∞çÈ°å„Äç„ÄÇ

**‚úÖ ‰øÆÊ≠£**: Áèæ‰ª£LLM (GPT-4o: 128k, Claude 3.5: 200k) ÂèØ‰ª•ËôïÁêÜÂÆåÊï¥ËçâÁ®øÔºö
```python
# ‰ΩøÁî®ÂÆåÊï¥ËçâÁ®øÊàñÊô∫ËÉΩÊà™Êñ∑
draft_for_planning = analyst_draft
if len(analyst_draft) > 10000:  # Âè™Âú®Ê•µÁ´ØÈï∑Â∫¶ÊôÇÊà™Êñ∑
    draft_for_planning = analyst_draft[:10000] + "\n\n[ËçâÁ®øÂ∑≤Êà™Êñ∑]"
```

### üî¥ Issue 2: PydanticÁπºÊâøÈô∑Èò±

**È¢®Èö™**: Â¶ÇÊûúÂïüÁî®feature flag‰ΩÜÁî®ÈåØË™§ÁöÑschema classÔºåÊñ∞Ê¨Ñ‰ΩçÊúÉË¢´ÈùúÈªò‰∏üÊ£Ñ„ÄÇ

**BugÁØÑ‰æã**:
```python
# ‚ùå ÈåØË™§ÔºöÂïüÁî®flag‰ΩÜÁî®Ëàäschema
enable_graphs = True
result = await call_llm_validated(
    prompt=prompt,
    response_schema=AnalystResearchOutput  # BUGÔºÅÊáâË©≤Áî®Enhanced
)
# result.argument_graph ÊúÉÈÅ∫Â§±
```

**‚úÖ ‰øÆÊ≠£**: ÂãïÊÖãÈÅ∏Êìáschema
```python
if enable_graphs:
    from reasoning.schemas_enhanced import AnalystResearchOutputEnhanced
    response_schema = AnalystResearchOutputEnhanced  # ‚úÖ
else:
    response_schema = AnalystResearchOutput
```

### üü° Issue 3: ÈÄ≤Â∫¶Ê¢ùÈÇèËºØÁ°¨Á∑®Á¢º

**ÂïèÈ°å**: ÈÄ≤Â∫¶Ê¨äÈáçÂØ´Ê≠ªÂú®ÂáΩÊï∏‰∏≠ÔºåÈõ£‰ª•Á∂≠Ë≠∑ÂíåË™øÊï¥„ÄÇ

**‚úÖ ‰øÆÊ≠£**: ÊèêÂèñÂà∞ÈÖçÁΩÆÈ°ûÂà•
```python
class ProgressConfig:
    STAGES = {
        "analyst_analyzing": {"weight": 0.3, "message": "Ê≠£Âú®Ê∑±Â∫¶ÂàÜÊûêË≥áÊñô‰æÜÊ∫ê..."},
        "critic_reviewing": {"weight": 0.6, "message": "Ê≠£Âú®Ê™¢Êü•ÈÇèËºØËàá‰æÜÊ∫êÂèØ‰ø°Â∫¶..."},
        # ...
    }
```

### ÂØ¶‰ΩúÂâçÊ™¢Êü•Ê∏ÖÂñÆ
- [ ] **Issue 1**: Â∞áÊâÄÊúâ`[:500]`ÊîπÁÇ∫`[:10000]`ÊàñÂÆåÊï¥ÂÇ≥ÂÖ•
- [ ] **Issue 2**: È©óË≠âÊâÄÊúâagentsÂú®feature flagsÂïüÁî®ÊôÇ‰ΩøÁî®Enhanced schemas
- [ ] **Issue 3**: Â∞áÈÄ≤Â∫¶ÈÖçÁΩÆÊèêÂèñÂà∞`ProgressConfig`È°ûÂà•

### Current Architecture Strengths to Preserve

From my exploration:
1. **Robust orchestrator** - 3-iteration Actor-Critic loop with gap detection and secondary search
2. **Clean agent separation** - Analyst/Critic/Writer with clear Pydantic schemas
3. **Hallucination guards** - Citation validation (`sources_used ‚äÜ analyst_citations`)
4. **SSE streaming** - Non-blocking progress updates via `_send_progress()`
5. **Retry logic** - `call_llm_validated()` with 3 retries and JSON repair

---

## Implementation Phases

### Recommended Sequence: Phase 1 ‚Üí 3 ‚Üí 2 ‚Üí 4

**Why not 1‚Üí2‚Üí3‚Üí4?**
- Phase 3 (Plan-and-Write) delivers immediate user value - better reports now
- Phase 2 (Argument Graphs) is more experimental and requires stabilization
- Phase 4 (KG UI) depends on Phase 2 backend stability

---

## Phase 1: User-Friendly SSE (Week 1, 3 days)

### Goal
Replace technical stage names (`"analyst_analyzing"`) with user-friendly Chinese messages (`"Ê≠£Âú®ÂàÜÊûêË≥áÊñô..."`).

### Files to Modify

#### 1. NEW: `code/python/reasoning/schemas_enhanced.py`
**Lines**: ~60 (new file)

```python
from pydantic import BaseModel, Field
from typing import Optional

class ProcessUpdate(BaseModel):
    """User-friendly progress message for SSE streaming."""
    stage: str = Field(..., description="Technical stage name (for backend)")
    user_message: str = Field(..., description="User-friendly Chinese message")
    progress: Optional[int] = Field(None, ge=0, le=100, description="Progress percentage")
```

#### 2. MODIFY: `code/python/reasoning/orchestrator.py`
**Current**: 865 lines
**Changes**: ~40 lines (enhanced `_send_progress()`)

**Location**: Around line 800-840 where `_send_progress()` is defined

**First, add ProgressConfig class** at top of orchestrator.py (around line 20):
```python
class ProgressConfig:
    """ÈÄ≤Â∫¶Ê¢ùÈÖçÁΩÆÔºåÁî®ÊñºSSE‰∏≤ÊµÅ„ÄÇ"""

    STAGES = {
        "analyst_analyzing": {
            "weight": 0.3,
            "message": "Ê≠£Âú®Ê∑±Â∫¶ÂàÜÊûêË≥áÊñô‰æÜÊ∫ê...",
        },
        "analyst_complete": {
            "weight": 0.5,
            "message": "ÂàÜÊûêÂÆåÊàêÔºåÈñãÂßãÂìÅË≥™ÂØ©Êü•",
        },
        "critic_reviewing": {
            "weight": 0.6,
            "message": "Ê≠£Âú®Ê™¢Êü•ÈÇèËºØËàá‰æÜÊ∫êÂèØ‰ø°Â∫¶...",
        },
        "critic_complete": {
            "weight": 0.8,
            "message": "ÂØ©Êü•ÂÆåÊàê",
        },
        "writer_planning": {
            "weight": 0.82,
            "message": "Ê≠£Âú®Ë¶èÂäÉÂ†±ÂëäÁµêÊßã...",
        },
        "writer_composing": {
            "weight": 0.85,
            "message": "Ê≠£Âú®Êí∞ÂØ´ÊúÄÁµÇÂ†±Âëä...",
        },
        "writer_complete": {
            "weight": 1.0,
            "message": "Â†±ÂëäÁîüÊàêÂÆåÊàê",
        },
        "gap_search_started": {
            "weight": 0.55,
            "message": "ÂÅµÊ∏¨Âà∞Ë≥áË®äÁº∫Âè£ÔºåÊ≠£Âú®Ë£úÂÖÖÊêúÂ∞ã...",
        }
    }

    @staticmethod
    def calculate_progress(stage: str, iteration: int, total_iterations: int) -> int:
        """Ë®àÁÆóÁµ¶ÂÆöstageÁöÑÈÄ≤Â∫¶ÁôæÂàÜÊØî„ÄÇ"""
        stage_info = ProgressConfig.STAGES.get(stage, {"weight": 0.5})
        base = int((iteration - 1) / total_iterations * 100)
        offset = int(stage_info["weight"] * (100 / total_iterations))
        return min(base + offset, 100)
```

**Then modify _send_progress()** around line 800-840:
```python
async def _send_progress(self, message: Dict[str, Any]) -> None:
    """Enhanced progress with user-friendly messages."""

    # Add user-friendly message based on stage (GeminiÂÑ™ÂåñÔºö‰ΩøÁî®ProgressConfig)
    if CONFIG.reasoning_params.get("features", {}).get("user_friendly_sse", False):
        stage = message.get("stage", "")
        iteration = message.get("iteration", 1)
        total = message.get("total_iterations", 3)

        # ‚úÖ ‰ΩøÁî®ÈÖçÁΩÆÈ°ûÂà•ËÄåÈùûÁ°¨Á∑®Á¢ºÂ≠óÂÖ∏
        stage_info = ProgressConfig.STAGES.get(stage)
        if stage_info:
            message["user_message"] = stage_info["message"]
            message["progress"] = ProgressConfig.calculate_progress(stage, iteration, total)

    # Existing send logic (unchanged)
    try:
        if hasattr(self.handler, 'message_sender'):
            await self.handler.message_sender.send_message(message)
    except Exception as e:
        self.logger.warning(f"Progress send failed: {e}")
```

#### 3. MODIFY: `config/config_reasoning.yaml`
**Current**: 43 lines
**Changes**: +8 lines

```yaml
reasoning:
  enabled: true
  max_iterations: 3
  analyst_timeout: 60
  critic_timeout: 30
  writer_timeout: 45

  # NEW: Feature flags
  features:
    user_friendly_sse: false  # Phase 1

  tracing:
    console:
      enabled: true
      level: DEBUG
```

#### 4. OPTIONAL: `static/news-search-prototype.html`
**Frontend update** to display `user_message` instead of raw `stage`.

**Location**: Find the SSE event listener (search for `addEventListener("message"`)

**Modification**:
```javascript
// BEFORE: evt.data.stage
// AFTER: evt.data.user_message || evt.data.stage
const displayMessage = data.user_message || data.stage;
```

### Testing
1. Run existing query: `Âè∞Á©çÈõªÈ´òÈõÑÂª†ÈÄ≤Â∫¶`
2. Verify SSE messages show Chinese text
3. Confirm progress percentage increases (0 ‚Üí 100)
4. **Regression test**: Disable flag, ensure old behavior works

### Rollout
- **Immediate 100%** (low risk, purely UI improvement)

---

## Phase 3: Plan-and-Write for Long Reports (Week 2-3, 7 days)

### Goal
Generate 2000+ word reports with structured sections instead of single-shot 500-word outputs.

### Files to Modify

#### 1. MODIFY: `code/python/reasoning/schemas_enhanced.py`
**Add** (~40 lines):

```python
class WriterPlanOutput(BaseModel):
    """Writer's outline plan before composition."""
    outline: str = Field(..., description="Markdown outline with section headers")
    estimated_length: int = Field(..., ge=1000, description="Target word count")
    key_arguments: List[str] = Field(
        default_factory=list,
        description="Core arguments to develop in each section"
    )

class WriterComposeOutputEnhanced(WriterComposeOutput):
    """Enhanced Writer output with optional plan metadata."""
    plan: Optional[WriterPlanOutput] = Field(
        default=None,
        description="Planning phase output (Phase 3 only)"
    )
```

#### 2. MODIFY: `code/python/reasoning/agents/writer.py`
**Current**: 399 lines
**Changes**: ~80 lines (20% addition)

**Add new method** `plan()` around line 100:

```python
async def plan(
    self,
    analyst_draft: str,
    critic_review: 'CriticReviewOutput',
    user_query: str,
    target_length: int = 2000
) -> 'WriterPlanOutput':
    """
    Generate outline plan for long-form report.

    Args:
        analyst_draft: The Analyst's draft (may be abbreviated)
        critic_review: Critic's feedback
        user_query: Original user query
        target_length: Target word count (default 2000)

    Returns:
        WriterPlanOutput with outline and key arguments
    """
    from reasoning.schemas_enhanced import WriterPlanOutput

    # GeminiÂÑ™ÂåñÔºö‰ΩøÁî®ÂÆåÊï¥ËçâÁ®øÊàñÊô∫ËÉΩÊà™Êñ∑ÔºåÈÅøÂÖç[:500]Êà™ÊéâÈóúÈçµË≥áË®ä
    draft_for_planning = analyst_draft
    if len(analyst_draft) > 10000:  # Âè™Âú®Ê•µÁ´ØÈï∑Â∫¶ÊôÇÊâçÊà™Êñ∑
        draft_for_planning = analyst_draft[:10000] + "\n\n[ËçâÁ®øÂ∑≤Êà™Êñ∑ÔºåÂÆåÊï¥ÁâàÊú¨Âú®Êí∞ÂØ´ÈöéÊÆµÊúÉ‰ΩøÁî®]"

    prompt = f"""‰Ω†ÊòØÂ†±ÂëäË¶èÂäÉÂ∞àÂÆ∂„ÄÇ

Ë´ãÊ†πÊìö‰ª•‰∏ãÂÖßÂÆπË®≠Ë®à‰∏ÄÂÄã {target_length} Â≠óÁöÑÊ∑±Â∫¶Â†±ÂëäÂ§ßÁ∂±Ôºö

### Analyst ËçâÁ®ø
{draft_for_planning}

### Critic ÂØ©Êü•ÊÑèË¶ã
{critic_review.critique}

### ‰ΩøÁî®ËÄÖÊü•Ë©¢
{user_query}

---

## ‰ªªÂãô

Ë´ãËº∏Âá∫ÁµêÊßãÂåñÁöÑÂ†±ÂëäÂ§ßÁ∂±ÔºàJSON Ê†ºÂºèÔºâÔºö

1. **Ê†∏ÂøÉË´ñÈªûË≠òÂà•**ÔºöÂæû Analyst ËçâÁ®ø‰∏≠ÊèêÂèñ 3-5 ÂÄãÊ†∏ÂøÉË´ñÈªû
2. **Á´†ÁØÄË¶èÂäÉ**ÔºöÁÇ∫ÊØèÂÄãË´ñÈªûÂàÜÈÖçÁ´†ÁØÄÔºå‰º∞ÁÆóÂ≠óÊï∏ÂàÜÈÖç
3. **Ë≠âÊìöÂàÜÈÖç**ÔºöÊ®ôË®ªÊØèÂÄãÁ´†ÁØÄÊáâ‰ΩøÁî®Âì™‰∫õÂºïÁî®‰æÜÊ∫ê

## Ëº∏Âá∫Ê†ºÂºè

```json
{{
  "outline": "# Â†±ÂëäÂ§ßÁ∂±\\n\\n## Á¨¨‰∏ÄÁ´†ÔºöËÉåÊôØËàáËÑàÁµ°\\n- È†ê‰º∞Â≠óÊï∏Ôºö400\\n- ‰ΩøÁî®‰æÜÊ∫êÔºö[1], [2]\\n\\n## Á¨¨‰∫åÁ´†ÔºöÊ†∏ÂøÉÁôºÁèæ\\n- È†ê‰º∞Â≠óÊï∏Ôºö800\\n- ‰ΩøÁî®‰æÜÊ∫êÔºö[3], [4], [5]\\n\\n## Á¨¨‰∏âÁ´†ÔºöÂΩ±ÈüøÂàÜÊûê\\n- È†ê‰º∞Â≠óÊï∏Ôºö600\\n- ‰ΩøÁî®‰æÜÊ∫êÔºö[6], [7]\\n\\n## ÁµêË´ñ\\n- È†ê‰º∞Â≠óÊï∏Ôºö200",
  "estimated_length": 2000,
  "key_arguments": ["Ë´ñÈªû A", "Ë´ñÈªû B", "Ë´ñÈªû C"]
}}
```

**Ë¶ÅÊ±Ç**Ôºö
- Â§ßÁ∂±ÂøÖÈ†àÊ∏ÖÊô∞„ÄÅÈÇèËºØÈÄ£Ë≤´
- Â≠óÊï∏ÂàÜÈÖçÂêàÁêÜÔºàÁ∏ΩÂíåÊé•ËøëÁõÆÊ®ôÂ≠óÊï∏Ôºâ
- Á´†ÁØÄÊï∏ÈáèÔºö3-5 Á´†
"""

    result = await self.call_llm_validated(
        prompt=prompt,
        response_schema=WriterPlanOutput,
        level="high"  # Use high quality for planning
    )

    self.logger.info(f"Plan generated: {len(result.key_arguments)} key arguments, est. {result.estimated_length} words")
    return result
```

**Modify existing** `compose()` method around line 150:

```python
async def compose(
    self,
    analyst_draft: str,
    critic_review: 'CriticReviewOutput',
    analyst_citations: List[int],
    mode: str,
    user_query: str,
    plan: Optional['WriterPlanOutput'] = None  # NEW parameter
):
    """
    Compose final report, optionally using pre-generated plan.

    Args:
        plan: Optional WriterPlanOutput from plan() method (Phase 3)
    """

    if plan:
        # Plan-and-Write mode (Phase 3)
        prompt = f"""‰Ω†ÊòØÂ†±ÂëäÊí∞ÂØ´Â∞àÂÆ∂„ÄÇ

Ë´ãÊ†πÊìö‰ª•‰∏ãÂ§ßÁ∂±Êí∞ÂØ´ÂÆåÊï¥Â†±ÂëäÔºàÁõÆÊ®ôÔºö{plan.estimated_length} Â≠óÔºâÔºö

### Â§ßÁ∂±
{plan.outline}

### ÂèØÁî®Á¥†Êùê
- Analyst ËçâÁ®øÔºö{analyst_draft}
- ÈóúÈçµË´ñÈªûÔºö{', '.join(plan.key_arguments)}
- ÂèØÁî®ÂºïÁî®ÔºàÁôΩÂêçÂñÆÔºâÔºö{analyst_citations}

### Ë¶ÅÊ±Ç
1. Âö¥Ê†ºÈÅµÂæ™Â§ßÁ∂±ÁµêÊßãÔºåÊØèÂÄãÁ´†ÁØÄÂÖÖÂàÜÂ±ïÈñã
2. ÊâÄÊúâÂºïÁî® **ÂøÖÈ†à** ‰æÜËá™ÁôΩÂêçÂñÆÔºö{analyst_citations}
3. Êèê‰æõÂÖ∑È´îË≠âÊìöÂíåÁ¥∞ÁØÄÔºåÈÅøÂÖçÁ©∫Ê¥ûË´ñËø∞
4. ÁõÆÊ®ôÂ≠óÊï∏Ôºö{plan.estimated_length} Â≠óÔºàÂÖÅË®± ¬±10%Ôºâ
5. ‰ΩøÁî® Markdown Ê†ºÂºèÔºåÂåÖÂê´Á´†ÁØÄÊ®ôÈ°åÔºà## Êàñ ###Ôºâ

## Ëº∏Âá∫Ê†ºÂºèÔºàJSONÔºâ

```json
{{
  "final_report": "# ÂÆåÊï¥Â†±Âëä\\n\\n## Á¨¨‰∏ÄÁ´†...\\n\\n...",
  "sources_used": [1, 3, 5],
  "confidence_level": "High",
  "methodology_note": "Âü∫Êñº {len(analyst_citations)} ÂÄã‰æÜÊ∫êÔºåÁ∂ìÈÅé 3 Ëº™ÂØ©Êü•"
}}
```
"""
        max_length = 8192  # Double token limit for long-form
    else:
        # Standard mode (existing prompt)
        prompt = self._build_compose_prompt(
            analyst_draft, critic_review, analyst_citations, mode, user_query
        )
        max_length = 4096

    # Call LLM (rest of method unchanged, just use max_length)
    # ... existing code ...
```

#### 3. MODIFY: `code/python/reasoning/orchestrator.py`
**Changes**: ~30 lines in Writer phase (around line 600-700)

**Location**: Find the Writer phase (search for `# Phase 3: Writer`)

**Modification**:
```python
# Phase 3: Writer
enable_plan_and_write = CONFIG.reasoning_params.get("features", {}).get("plan_and_write", False)

if enable_plan_and_write:
    # Step 1: Plan
    await self._send_progress({
        "message_type": "intermediate_result",
        "stage": "writer_planning",
        "iteration": iteration,
        "total_iterations": self.max_iterations
    })

    plan = await self.writer.plan(
        analyst_draft=draft,
        critic_review=review,
        user_query=query,
        target_length=2000
    )

    # Step 2: Compose
    await self._send_progress({
        "message_type": "intermediate_result",
        "stage": "writer_composing",
        "iteration": iteration,
        "total_iterations": self.max_iterations
    })

    result = await self.writer.compose(
        analyst_draft=draft,
        critic_review=review,
        analyst_citations=analyst_citations,
        mode=mode,
        user_query=query,
        plan=plan  # Pass plan
    )
else:
    # Standard single-step compose (existing code)
    result = await self.writer.compose(...)
```

#### 4. MODIFY: `config/config_reasoning.yaml`
**Add**:

```yaml
reasoning:
  features:
    user_friendly_sse: false
    plan_and_write: false  # NEW: Phase 3

  writer_timeout: 90  # INCREASE from 45s to 90s for long-form generation
```

### Testing
1. **Shadow mode** (Week 2): Generate plan but don't use it, log to iteration_logger
2. **Test queries** (Week 3):
   - Simple: "Âè∞Á©çÈõªÈ´òÈõÑÂª†ÈÄ≤Â∫¶" (baseline, ~500 words expected)
   - Complex: "ÂàÜÊûêÂè∞Á©çÈõª2020-2024Âπ¥ÊäÄË°ìÊºîÈÄ≤ËàáÂ∏ÇÂ†¥Á≠ñÁï•" (2000+ words expected)
3. **Validation**:
   - Word count: 1800-2200
   - Section headers present
   - No citation hallucinations (`sources_used ‚äÜ analyst_citations`)
4. **A/B comparison**: Quality review (blind test) vs. standard mode

### Rollout
- Week 2: Shadow mode (log only)
- Week 3: 10% traffic
- Week 4: 50% if quality > baseline
- Week 5: 100% if no regressions

---

## Phase 2: Argument Graphs (Week 4-6, 10 days)

### Goal
Structured logical decomposition with ArgumentNode and WeaknessType validation.

### Files to Modify

#### 1. MODIFY: `code/python/reasoning/schemas_enhanced.py`
**Add** (~100 lines):

```python
from enum import Enum
import uuid

class LogicType(str, Enum):
    """Types of logical reasoning."""
    DEDUCTION = "deduction"  # ÊºîÁππÔºöÂæûÊôÆÈÅçÂéüÂâáÊé®Â∞é
    INDUCTION = "induction"  # Ê≠∏Á¥çÔºöÂæûÂ§öÂÄãÊ°à‰æãÁ∏ΩÁµê
    ABDUCTION = "abduction"  # Ê∫ØÂõ†ÔºöÂæûÁµêÊûúÊé®Ê∏¨ÂéüÂõ†

class WeaknessType(str, Enum):
    """Fixed vocabulary for logical weakness detection."""
    INSUFFICIENT_EVIDENCE = "insufficient_evidence"
    BIASED_SAMPLE = "biased_sample"
    CORRELATION_NOT_CAUSATION = "correlation_not_causation"
    HASTY_GENERALIZATION = "hasty_generalization"
    MISSING_ALTERNATIVES = "missing_alternatives"
    INVALID_DEDUCTION = "invalid_deduction"
    SOURCE_TIER_VIOLATION = "source_tier_violation"
    LOGICAL_LEAP = "logical_leap"

class ArgumentNode(BaseModel):
    """Single logical unit in reasoning chain."""
    node_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    claim: str = Field(..., description="The logical claim being made")
    evidence_ids: List[int] = Field(
        default_factory=list,
        description="Citation IDs supporting this claim (e.g., [1, 3])"
    )
    reasoning_type: LogicType = LogicType.INDUCTION
    confidence: Literal["high", "medium", "low"] = "medium"

class StructuredWeakness(BaseModel):
    """Critic's structured weakness detection."""
    node_id: str = Field(..., description="UUID of affected ArgumentNode")
    weakness_type: WeaknessType
    severity: Literal["critical", "moderate", "minor"] = "moderate"
    explanation: str = Field(..., min_length=20, description="Why this is a weakness")

# Enhanced schemas
class AnalystResearchOutputEnhanced(AnalystResearchOutput):
    """Analyst output with optional argument graph."""
    argument_graph: Optional[List[ArgumentNode]] = Field(
        default=None,
        description="Structured argument decomposition (Phase 2)"
    )

class CriticReviewOutputEnhanced(CriticReviewOutput):
    """Critic output with optional structured weaknesses."""
    structured_weaknesses: Optional[List[StructuredWeakness]] = Field(
        default=None,
        description="Structured weakness analysis (Phase 2)"
    )
```

#### 2. MODIFY: `code/python/reasoning/agents/analyst.py`
**Current**: 401 lines
**Changes**: ~60 lines (15% addition)

**Modify** `research()` method around line 100:

```python
async def research(self, query, formatted_context, mode, temporal_context=None):
    """Enhanced research with optional argument graph generation."""

    # Check feature flag
    enable_graphs = CONFIG.reasoning_params.get("features", {}).get("argument_graphs", False)

    # Build prompt with optional graph instructions
    prompt = self._build_research_prompt(
        query, formatted_context, mode, temporal_context,
        enable_argument_graph=enable_graphs  # NEW parameter
    )

    # Choose schema based on feature flag
    if enable_graphs:
        from reasoning.schemas_enhanced import AnalystResearchOutputEnhanced
        response_schema = AnalystResearchOutputEnhanced
    else:
        response_schema = AnalystResearchOutput

    result = await self.call_llm_validated(prompt, response_schema, level="high")

    # Validate argument graph if present
    if hasattr(result, 'argument_graph') and result.argument_graph:
        self._validate_argument_graph(result.argument_graph, result.citations_used)

    return result

def _validate_argument_graph(self, graph: List['ArgumentNode'], valid_citations: List[int]) -> None:
    """Ensure argument graph cites only available sources."""
    for node in graph:
        invalid = [eid for eid in node.evidence_ids if eid not in valid_citations]
        if invalid:
            self.logger.warning(f"Node {node.node_id[:8]} has invalid evidence_ids: {invalid}")
            # Remove invalid citations
            node.evidence_ids = [eid for eid in node.evidence_ids if eid in valid_citations]
```

**Modify** `_build_research_prompt()` around line 200:

```python
def _build_research_prompt(self, ..., enable_argument_graph=False):
    # ... existing prompt construction ...

    if enable_argument_graph:
        graph_instructions = """
---

## ÈöéÊÆµ 2.5ÔºöÁü•Ë≠òÂúñË≠úÂª∫ÊßãÔºàÁµêÊßãÂåñËº∏Âá∫Ôºâ

Èô§‰∫ÜÂéüÊúâÁöÑ JSON Ê¨Ñ‰ΩçÂ§ñÔºåÊñ∞Â¢û `argument_graph` Ê¨Ñ‰ΩçÔºàÈô£ÂàóÔºâÔºö

```json
{
  "status": "DRAFT_READY",
  "draft": "...",
  "reasoning_chain": "...",
  "citations_used": [1, 3, 5],
  "argument_graph": [
    {
      "claim": "Âè∞Á©çÈõªÈ´òÈõÑÂª†Âª∂ÂæåËá≥2026Âπ¥ÈáèÁî¢",
      "evidence_ids": [1, 3],
      "reasoning_type": "induction",
      "confidence": "high"
    },
    {
      "claim": "Âª∂ÂæåÂéüÂõ†ÂèØËÉΩÊòØË®≠ÂÇô‰æõÊáâÈèàÂïèÈ°å",
      "evidence_ids": [3],
      "reasoning_type": "abduction",
      "confidence": "medium"
    }
  ]
}
```

### Ë¶èÂâá

1. **ÊØèÂÄãÈóúÈçµË´ñÈªûÈÉΩÊòØ‰∏ÄÂÄã node**
2. **evidence_ids ÂøÖÈ†àÊòØ citations_used ÁöÑÂ≠êÈõÜ**
3. **reasoning_type ÈÅ∏Êìá**Ôºö
   - `deduction`: Âü∫ÊñºÊôÆÈÅçÂéüÂâáÊé®Â∞éÔºàÂ¶ÇÊ≥ïÂæã„ÄÅÁâ©ÁêÜÂÆöÂæãÔºâ
   - `induction`: Âü∫ÊñºÂ§öÂÄãÊ°à‰æãÊ≠∏Á¥çÔºàÂ¶ÇË∂®Âã¢ÂàÜÊûêÔºâ
   - `abduction`: Âü∫ÊñºËßÄÂØüÊé®Ê∏¨ÂéüÂõ†ÔºàÂ¶ÇËß£ÈáãÁèæË±°Ôºâ
4. **confidence Âü∫ÊñºË≠âÊìöÂäõ**Ôºö
   - `high`: Tier 1-2 ‰æÜÊ∫ê + Â§öÂÄãÁç®Á´ãË≠âÂØ¶
   - `medium`: ÂñÆ‰∏Ä Tier 2 ÊàñÂ§öÂÄã Tier 3
   - `low`: ÂÉÖÊúâ Tier 4-5 ÊàñÊé®Ê∏¨ÊÄßÈô≥Ëø∞

**ÈáçË¶Å**ÔºöÂ¶ÇÊûúË≥áÊñô‰∏çË∂≥‰ª•Âª∫ÊßãÂúñË≠úÔºåÂèØ‰ª•Â∞á `argument_graph` Ë®≠ÁÇ∫ `null` ÊàñÁ©∫Èô£Âàó `[]`„ÄÇÁ≥ªÁµ±ÊúÉÊ≠£Â∏∏ÈÅã‰Ωú„ÄÇ
"""
        prompt += graph_instructions

    return prompt
```

#### 3. MODIFY: `code/python/reasoning/agents/critic.py`
**Current**: 365 lines
**Changes**: ~70 lines (19% addition)

**Modify** `review()` method:

```python
async def review(self, analyst_output, query, mode):
    """Enhanced review with optional structured weaknesses."""

    enable_structured = CONFIG.reasoning_params.get("features", {}).get("structured_critique", False)

    # Build prompt
    prompt = self._build_review_prompt(
        draft=analyst_output.draft,
        argument_graph=getattr(analyst_output, 'argument_graph', None),
        query=query,
        mode=mode,
        enable_structured_weaknesses=enable_structured
    )

    # Choose schema
    if enable_structured:
        from reasoning.schemas_enhanced import CriticReviewOutputEnhanced
        response_schema = CriticReviewOutputEnhanced
    else:
        response_schema = CriticReviewOutput

    result = await self.call_llm_validated(prompt, response_schema, level="high")

    # Auto-escalate based on critical weaknesses
    if hasattr(result, 'structured_weaknesses') and result.structured_weaknesses:
        critical_count = sum(1 for w in result.structured_weaknesses if w.severity == "critical")
        thresholds = CONFIG.reasoning_params.get("critique_thresholds", {})
        max_critical = thresholds.get("critical_weakness_count", 2)

        if critical_count >= max_critical and result.status != "REJECT":
            self.logger.warning(f"Auto-escalating to REJECT: {critical_count} critical weaknesses")
            # Rebuild with REJECT (Pydantic immutable)
            result = CriticReviewOutputEnhanced(
                status="REJECT",
                critique=result.critique + f"\n\n[Ëá™ÂãïÂçáÁ¥öËá≥ REJECTÔºö{critical_count} ÂÄãÂö¥ÈáçÂïèÈ°å]",
                suggestions=result.suggestions,
                mode_compliance=result.mode_compliance,
                logical_gaps=result.logical_gaps,
                source_issues=result.source_issues,
                structured_weaknesses=result.structured_weaknesses
            )

    return result
```

**Add to** `_build_review_prompt()`:

```python
def _build_review_prompt(self, ..., argument_graph=None, enable_structured_weaknesses=False):
    # ... existing prompt ...

    if enable_structured_weaknesses and argument_graph:
        weakness_instructions = """
---

## Âº±ÈªûÂàÜÈ°ûÔºàWeaknessTypeÔºâ

Ë´ãÈáùÂ∞çÊØèÂÄã ArgumentNode Ê™¢Êü•‰ª•‰∏ãÊ®ôÊ∫ñÂº±ÈªûÔºàÂøÖÈ†àÂÆåÂÖ®ÂåπÈÖçÔºâÔºö

- `"insufficient_evidence"`: Ë≠âÊìö‰∏çË∂≥ÔºàÂÉÖ 1 ÂÄã‰æÜÊ∫êÊîØÊåÅÈóúÈçµË´ñÈªûÔºâ
- `"biased_sample"`: Ê®£Êú¨ÂÅèË™§ÔºàÂè™ÂºïÁî®ÊàêÂäüÊ°à‰æãÔºåÂøΩÁï•Â§±ÊïóÊ°à‰æãÔºâ
- `"correlation_not_causation"`: Áõ∏ÈóúÈùûÂõ†ÊûúÔºàË™§Â∞áÁõ∏ÈóúÊÄßÁï∂Âõ†ÊûúÔºâ
- `"hasty_generalization"`: ÂÄâ‰øÉÊ≠∏Á¥çÔºàÂ∞èÊ®£Êú¨Êé®Âª£Ëá≥ÂÖ®È´îÔºâ
- `"missing_alternatives"`: Áº∫Â∞ëÊõø‰ª£Ëß£ÈáãÔºàabduction Âè™Êèê 1 Á®ÆÂèØËÉΩÔºâ
- `"invalid_deduction"`: ÁÑ°ÊïàÊºîÁππÔºàÂâçÊèê‰∏çÊîØÊåÅÁµêË´ñÔºâ
- `"source_tier_violation"`: ‰æÜÊ∫êÂ±§Á¥öÈÅïË¶èÔºàstrict mode ÂºïÁî® Tier 3+Ôºâ
- `"logical_leap"`: ÈÇèËºØË∑≥Ë∫çÔºàÁº∫Â∞ë‰∏≠ÈñìÊé®ÁêÜÊ≠•È©üÔºâ

**Ëº∏Âá∫ÁØÑ‰æã**Ôºö

```json
{
  "status": "REJECT",
  "critique": "...",
  "suggestions": ["..."],
  "mode_compliance": "ÈÅïÂèç",
  "logical_gaps": ["..."],
  "source_issues": ["..."],
  "structured_weaknesses": [
    {
      "node_id": "uuid-from-analyst",
      "weakness_type": "source_tier_violation",
      "severity": "critical",
      "explanation": "Âú® strict Ê®°Âºè‰∏ãÂºïÁî®‰∫Ü Dcard (Tier 5)ÔºåÈÅïÂèç max_tier=2 Ë¶èÂâá"
    }
  ]
}
```
"""
        prompt += weakness_instructions

    return prompt
```

#### 4. MODIFY: `config/config_reasoning.yaml`
**Add**:

```yaml
reasoning:
  features:
    user_friendly_sse: false
    plan_and_write: false
    argument_graphs: false       # NEW: Phase 2
    structured_critique: false   # NEW: Phase 2

  # NEW: Auto-REJECT thresholds
  critique_thresholds:
    critical_weakness_count: 2
    source_tier_violations: 1
```

### Testing
1. **Unit tests**: ArgumentNode validation, WeaknessType enum
2. **LLM parsing tests**: Mock responses, verify JSON parsing
3. **End-to-end tests** with 5 query types:
   - Deductive: "Ê†πÊìöÂÖ¨Âè∏Ê≥ïÔºåÂè∞Á©çÈõªËë£‰∫ãÊúÉÊ±∫Ë≠∞ÈúÄË¶ÅÂ§öÂ∞ë‰∫∫ÂêåÊÑèÔºü"
   - Inductive: "2024Âπ¥AIÊô∂ÁâáÈúÄÊ±ÇË∂®Âã¢"
   - Abductive: "ÁÇ∫‰ªÄÈ∫ºÂè∞Á©çÈõªÈ´òÈõÑÂª†Âª∂ÂæåÔºü"
   - Edge case: "PTTÈÑâÊ∞ëË™™Âè∞Á©çÈõªË¶ÅÂÄíÈñâ" (expect source_tier_violation)
4. **Monitoring**: LLM parsing success rate (target >90%)

### Rollout
- Week 4: Backend only, log graphs to iteration_logger
- Week 5: 10% traffic, monitor parsing errors
- Week 6: 50% if parsing success >90%
- Week 7+: Frontend graph visualization (stretch goal)

---

## Phase 4: Knowledge Graph UI (Future, Week 8+)

### Goal
Enable user editing of argument graphs for personalized reasoning.

### Tasks (Deferred)
- Design graph UI (D3.js or Cytoscape.js)
- API endpoint to expose argument graphs
- User interaction (click to edit nodes, add/remove edges)
- KG storage layer (PostgreSQL or graph DB)
- Prompt integration: "User has marked 'A implies B' as FALSE..."

**Note**: Phase 4 depends on Phase 2 stability. Can be delayed until Phase 2 adoption stabilizes.

---

## Critical Files Summary

### Phase 1 (3 files)
1. **NEW**: `code/python/reasoning/schemas_enhanced.py` (~60 lines)
2. **MODIFY**: `code/python/reasoning/orchestrator.py` (+40 lines)
3. **MODIFY**: `config/config_reasoning.yaml` (+8 lines)

### Phase 3 (3 files)
1. **MODIFY**: `code/python/reasoning/schemas_enhanced.py` (+40 lines)
2. **MODIFY**: `code/python/reasoning/agents/writer.py` (+80 lines)
3. **MODIFY**: `code/python/reasoning/orchestrator.py` (+30 lines in Writer phase)

### Phase 2 (4 files)
1. **MODIFY**: `code/python/reasoning/schemas_enhanced.py` (+100 lines)
2. **MODIFY**: `code/python/reasoning/agents/analyst.py` (+60 lines)
3. **MODIFY**: `code/python/reasoning/agents/critic.py` (+70 lines)
4. **MODIFY**: `config/config_reasoning.yaml` (+6 lines)

**Total**: 1 new file, 4 modified files across all phases.

---

## Risk Mitigation

### 1. LLM Generates Invalid Graph JSON
**Mitigation**:
- Existing retry logic (3 attempts) + JSON repair (`safe_parse_llm_json()`)
- Fallback: `argument_graph=None`, log warning, continue with markdown
- Prompt engineering: Explicit JSON examples

### 2. Token Budget Exceeded
**Mitigation**:
- Increase `max_length` to 8192 for Plan-and-Write
- Two-step process splits token usage
- Prompt: "ÂÑ™ÂÖàÂÆåÊàê JSON ÁµêÊßã"

### 3. Critic Auto-REJECT Too Aggressive
**Mitigation**:
- Configurable thresholds (start at 2 critical weaknesses)
- Track rejection rate analytics
- Tune based on first 100 queries

### 4. Performance Impact
**Mitigation**:
- A/B test to ensure quality improvement justifies latency
- Measure baseline before rollout
- Future optimization: parallel plan generation

---

## Rollback Plan

### Immediate (<5 min)
```yaml
# Flip feature flags to false
reasoning:
  features:
    user_friendly_sse: false
    plan_and_write: false
    argument_graphs: false
    structured_critique: false
```

### Emergency Code Rollback
```bash
git revert HEAD~3..HEAD
git push origin main
./deploy.sh
```

**Recovery Time**: ~10 minutes

---

## Success Criteria

### Phase 1
- [ ] SSE shows Chinese messages instead of technical stages
- [ ] Progress percentage increases 0 ‚Üí 100
- [ ] All existing tests pass

### Phase 3
- [ ] Reports average 2000+ words (vs. current ~500)
- [ ] Clear section structure with Markdown headers
- [ ] No citation hallucinations
- [ ] A/B test: Quality > baseline

### Phase 2
- [ ] 80%+ queries generate non-empty argument graphs
- [ ] LLM parsing success >90%
- [ ] Critic correctly identifies weakness types
- [ ] Graph generation latency <20% overhead

---

## Next Steps

1. **Review this plan** with user - clarify any questions
2. **Begin Phase 1** (Week 1) - User-friendly SSE
3. **Schedule weekly reviews** to track progress and adjust
4. **Document findings** in iteration logs for future optimization

---

## Open Questions for User

1. **Implementation priority**: Should we start with Phase 3 (Plan-and-Write) or Phase 1 (SSE)?
   - Recommendation: Phase 1 first (3 days, low risk, quick win)

2. **LLM cost control**: Use same model for all agents or tiered strategy?
   - Recommendation: Same model initially, optimize later if cost is issue

3. **Graph UI timeline**: Implement simple text display in Phase 2 or wait for Phase 4 full visualization?
   - Recommendation: Wait for Phase 4 (focus backend quality first)

4. **Rollout speed**: Conservative (10% ‚Üí 50% ‚Üí 100% over 3 weeks) or aggressive (100% immediate)?
   - Recommendation: Conservative for Phase 2-3, aggressive for Phase 1
