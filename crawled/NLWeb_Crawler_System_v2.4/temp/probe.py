"""
probe.py - è‡ªå‹•åŒ–ç¶²ç«™åµæŸ¥è…³æœ¬ï¼ˆæ”¹é€²ç‰ˆ v3ï¼‰

Phase 0 - æˆ°å ´åµæŸ¥ (Reconnaissance)

æ”¹é€²ï¼ˆv3ï¼‰ï¼š
- å¢åŠ  URL æ¸…ç†é‚è¼¯ï¼ˆè™•ç†ç›¸å°è·¯å¾‘ï¼‰
- æŠ“å–æ‰€æœ‰æ–‡ç« é€£çµï¼ˆä¸é™åˆ¶ 5 å€‹ï¼‰
- ä¿®å¾©é˜²ç¦¦æ¸¬è©¦å’Œå…§å®¹é©—è­‰

åŸ·è¡Œæ–¹å¼ï¼š
python temp/probe.py
"""

import asyncio
import re
import json
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from urllib.parse import urlparse, parse_qs, urljoin
from bs4 import BeautifulSoup

# ==================== HTTP å®¢æˆ¶ç«¯ ====================

try:
    import aiohttp
    AIOHTTP_AVAILABLE = True
except ImportError:
    AIOHTTP_AVAILABLE = False
    print("âš ï¸  aiohttp æœªå®‰è£")

try:
    from curl_cffi.requests import AsyncSession
    CURL_CFFI_AVAILABLE = True
except ImportError:
    CURL_CFFI_AVAILABLE = False
    print("âš ï¸  curl_cffi æœªå®‰è£ (pip install curl_cffi)")


# ==================== HTTP Headers ====================
DEFAULT_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept": (
        "text/html,application/xhtml+xml,application/xml;q=0.9,"
        "image/avif,image/webp,image/apng,*/*;q=0.8"
    ),
    "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7",
    "Accept-Encoding": "gzip, deflate, br",
    "Referer": "https://www.google.com/",
    "Connection": "keep-alive",
}


# ==================== è‡ªå‹•åŒ–åµæŸ¥å·¥å…· ====================

class AutoScout:
    """è‡ªå‹•åŒ–åµæŸ¥å…µï¼ˆæ”¹é€²ç‰ˆ v3ï¼‰"""
    
    def __init__(self, home_url: str):
        self.home_url = home_url.rstrip('/')
        self.base_domain = self._extract_domain(home_url)
        
        self.report = {
            "home_url": home_url,
            "base_domain": self.base_domain,
            "categories": {},
            "subdomains": [],
            "list_page": None,
            "sample_articles": [],
            "url_pattern": None,
            "turbo_mode": False,
            "defense_test": {},
            "content_validation": {}
        }
    
    def _extract_domain(self, url: str) -> str:
        """æå–ä¸»ç¶²åŸŸ"""
        match = re.search(r'https?://(?:www\.)?([^/]+)', url)
        return match.group(1) if match else ""
    
    def _normalize_url(self, url: str, base_url: str) -> str:
        """
        æ¸…ç†å’Œæ¨™æº–åŒ– URL
        
        è™•ç†ï¼š
        - ç›¸å°è·¯å¾‘ (../, ./)
        - çµ•å°è·¯å¾‘ (/path)
        - å®Œæ•´ URL (http://...)
        """
        # ä½¿ç”¨ urljoin è‡ªå‹•è™•ç†ç›¸å°è·¯å¾‘
        normalized = urljoin(base_url, url)
        return normalized
    
    # ==================== éšæ®µ 1ï¼šå…¨è‡ªå‹•åµæŸ¥ ====================
    
    async def auto_reconnaissance(self):
        """å…¨è‡ªå‹•åµæŸ¥æµç¨‹"""
        print("\n" + "="*70)
        print("ğŸ¤– éšæ®µ 2ï¼šå…¨è‡ªå‹•åµæŸ¥")
        print("="*70)
        
        await self._analyze_homepage()
        await self._find_list_page()
        await self._extract_sample_articles()
        await self._analyze_url_pattern()
        await self._test_defenses()
        
        print("\n" + "="*70)
        print("âœ… è‡ªå‹•åµæŸ¥å®Œæˆ")
        print("="*70)
    
    async def _analyze_homepage(self):
        """åˆ†æé¦–é """
        print("\nã€æ­¥é©Ÿ 1/5ã€‘åˆ†æé¦–é ...")
        print("-" * 70)
        
        html = await self._fetch_html(self.home_url)
        
        if not html:
            print("âŒ ç„¡æ³•æŠ“å–é¦–é ")
            return
        
        print(f"âœ… æˆåŠŸæŠ“å–é¦–é  ({len(html)} bytes)")
        
        soup = BeautifulSoup(html, 'lxml')
        
        print("\n1ï¸âƒ£ å°‹æ‰¾åˆ†é¡...")
        categories = self._find_categories(soup)
        
        if categories:
            self.report['categories'] = categories
            print(f"   âœ… æ‰¾åˆ° {len(categories)} å€‹åˆ†é¡")
            for i, (code, info) in enumerate(list(categories.items())[:5], 1):
                print(f"      {i}. {info['name']:12s} -> {code}")
        else:
            print("   âš ï¸  æœªæ‰¾åˆ°åˆ†é¡")
        
        print("\n2ï¸âƒ£ æª¢æŸ¥å­ç¶²åŸŸ...")
        subdomains = self._find_subdomains(soup)
        
        if subdomains:
            self.report['subdomains'] = list(subdomains)
            print(f"   âš ï¸  ç™¼ç¾ {len(subdomains)} å€‹å­ç¶²åŸŸ")
            for subdomain in list(subdomains)[:3]:
                print(f"      - {subdomain}")
        else:
            print("   âœ… ç„¡å­ç¶²åŸŸé™·é˜±")
    
    async def _find_list_page(self):
        """å°‹æ‰¾åˆ—è¡¨é """
        print("\nã€æ­¥é©Ÿ 2/5ã€‘å°‹æ‰¾åˆ—è¡¨é ...")
        print("-" * 70)
        
        html = await self._fetch_html(self.home_url)
        
        if html:
            soup = BeautifulSoup(html, 'lxml')
            
            keywords = ['å³æ™‚', 'æœ€æ–°', 'å…¨éƒ¨', 'latest', 'all', 'news', 'æ–°è']
            for link in soup.find_all('a', href=True):
                text = link.get_text(strip=True).lower()
                href = link['href']
                
                if any(kw in text for kw in keywords):
                    # ä½¿ç”¨ _normalize_url è™•ç†ç›¸å°è·¯å¾‘
                    href = self._normalize_url(href, self.home_url)
                    
                    if await self._validate_list_page(href):
                        self.report['list_page'] = href
                        print(f"âœ… æ‰¾åˆ°åˆ—è¡¨é : {href}")
                        print(f"   é—œéµå­—: {text}")
                        return
        
        print("   å˜—è©¦å¸¸è¦‹è·¯å¾‘...")
        common_paths = [
            '/list/aall.aspx',
            '/list/all.aspx',
            '/list/',
            '/news/',
            '/latest/',
            '/all/',
            '/article/list/',
        ]
        
        for path in common_paths:
            test_url = self.home_url + path
            
            if await self._validate_list_page(test_url):
                self.report['list_page'] = test_url
                print(f"âœ… æ‰¾åˆ°åˆ—è¡¨é : {test_url}")
                return
        
        if self.report['categories']:
            print("   å¾åˆ†é¡ä¸­å°‹æ‰¾...")
            for code, info in list(self.report['categories'].items())[:3]:
                url = info.get('url')
                if url and await self._validate_list_page(url):
                    self.report['list_page'] = url
                    print(f"âœ… æ‰¾åˆ°åˆ—è¡¨é : {url}")
                    print(f"   åˆ†é¡: {info['name']}")
                    return
        
        print("âŒ ç„¡æ³•è‡ªå‹•æ‰¾åˆ°åˆ—è¡¨é ")
    
    async def _extract_sample_articles(self):
        """å¾åˆ—è¡¨é æå–ç¯„ä¾‹æ–‡ç« ï¼ˆæ”¹é€²ç‰ˆï¼šæŠ“å–æ‰€æœ‰é€£çµï¼‰"""
        print("\nã€æ­¥é©Ÿ 3/5ã€‘æå–ç¯„ä¾‹æ–‡ç« ...")
        print("-" * 70)
        
        if not self.report['list_page']:
            print("âš ï¸  æ²’æœ‰åˆ—è¡¨é ï¼Œç„¡æ³•æå–ç¯„ä¾‹")
            return
        
        html = await self._fetch_html(self.report['list_page'])
        
        if not html:
            print("âŒ ç„¡æ³•æŠ“å–åˆ—è¡¨é ")
            return
        
        soup = BeautifulSoup(html, 'lxml')
        
        article_links = []
        seen_urls = set()  # é¿å…é‡è¤‡
        
        # è·¯å¾‘æ ¼å¼æ¨¡å¼
        path_patterns = [
            r'/news/[^/]+/\d+',
            r'/article/\d+',
            r'/\d+\.html',
        ]
        
        for link in soup.find_all('a', href=True):
            href = link['href']
            
            # æª¢æŸ¥è·¯å¾‘æ ¼å¼
            if any(re.search(pattern, href) for pattern in path_patterns):
                # âœ¨ ä½¿ç”¨ _normalize_url æ¸…ç† URL
                clean_url = self._normalize_url(href, self.report['list_page'])
                
                if clean_url not in seen_urls:
                    article_links.append(clean_url)
                    seen_urls.add(clean_url)
            
            # æª¢æŸ¥ Query String æ ¼å¼
            elif self._has_article_id(href):
                # âœ¨ ä½¿ç”¨ _normalize_url æ¸…ç† URL
                clean_url = self._normalize_url(href, self.report['list_page'])
                
                if clean_url not in seen_urls:
                    article_links.append(clean_url)
                    seen_urls.add(clean_url)
        
        if article_links:
            self.report['sample_articles'] = article_links
            print(f"âœ… æå– {len(article_links)} å€‹ç¯„ä¾‹æ–‡ç« ")
            
            # é¡¯ç¤ºå‰ 5 å€‹ç¯„ä¾‹
            display_count = min(5, len(article_links))
            for i, url in enumerate(article_links[:display_count], 1):
                print(f"   {i}. {url}")
            
            if len(article_links) > 5:
                print(f"   ... é‚„æœ‰ {len(article_links) - 5} å€‹")
        else:
            print("âŒ åˆ—è¡¨é ä¸­æ²’æœ‰æ‰¾åˆ°æ–‡ç« é€£çµ")
    
    def _has_article_id(self, url: str) -> bool:
        """
        æª¢æŸ¥ URL æ˜¯å¦åŒ…å«æ–‡ç«  ID
        
        æ”¯æ´æ ¼å¼ï¼š
        - Query String: ?news_id=123, ?id=123, ?article_id=123
        - è·¯å¾‘: /news/123, /article/123
        """
        # æª¢æŸ¥ Query String
        parsed = urlparse(url)
        params = parse_qs(parsed.query)
        
        # å¸¸è¦‹çš„ ID åƒæ•¸åç¨±
        id_params = ['news_id', 'id', 'article_id', 'post_id', 'nid', 'aid']
        
        for param in id_params:
            if param in params:
                return True
        
        # æª¢æŸ¥è·¯å¾‘ä¸­çš„æ•¸å­—
        if re.search(r'/\d+', parsed.path):
            return True
        
        return False
    
    async def _analyze_url_pattern(self):
        """åˆ†æ URL çµæ§‹ï¼ˆæ”¹é€²ç‰ˆï¼šæ”¯æ´ Query Stringï¼‰"""
        print("\nã€æ­¥é©Ÿ 4/5ã€‘åˆ†æ URL çµæ§‹...")
        print("-" * 70)
        
        if not self.report['sample_articles']:
            print("âš ï¸  æ²’æœ‰ç¯„ä¾‹æ–‡ç« ï¼Œç„¡æ³•åˆ†æ URL çµæ§‹")
            return
        
        print(f"åˆ†æ {len(self.report['sample_articles'])} å€‹ç¯„ä¾‹...")
        
        patterns_found = []
        
        for url in self.report['sample_articles']:
            pattern = self._extract_url_pattern(url)
            if pattern:
                patterns_found.append(pattern)
        
        if not patterns_found:
            print("âŒ ç„¡æ³•è§£æ URL çµæ§‹")
            return
        
        # çµ±è¨ˆæœ€å¸¸è¦‹çš„æ¨¡å¼
        pattern_counts = {}
        for p in patterns_found:
            key = p['pattern']
            pattern_counts[key] = pattern_counts.get(key, 0) + 1
        
        most_common = max(pattern_counts, key=pattern_counts.get)
        
        print(f"âœ… URL çµæ§‹: {most_common}")
        print(f"   åŒ¹é…æ•¸é‡: {pattern_counts[most_common]}/{len(patterns_found)}")
        self.report['url_pattern'] = most_common
        
        # æå–æ‰€æœ‰ ID
        ids = [p.get('id') for p in patterns_found if p.get('id')]
        
        if ids:
            print(f"\nğŸ“Š ID åˆ†æ ({len(ids)} å€‹):")
            
            # é¡¯ç¤ºç¯„ä¾‹ ID
            display_count = min(5, len(ids))
            for i, id_val in enumerate(ids[:display_count], 1):
                print(f"   {i}. {id_val}")
            
            if len(ids) > 5:
                print(f"   ... é‚„æœ‰ {len(ids) - 5} å€‹")
            
            id_lengths = [len(str(id_val)) for id_val in ids]
            if len(set(id_lengths)) == 1:
                id_length = id_lengths[0]
                print(f"\n   âœ… ID é•·åº¦ä¸€è‡´: {id_length} ä½æ•¸")
                
                if id_length >= 8:
                    print(f"\n   å˜—è©¦å¾ ID æå–æ—¥æœŸ...")
                    date_success = 0
                    
                    for id_val in ids[:3]:
                        id_str = str(id_val)
                        date_part = id_str[:8]
                        
                        try:
                            date_obj = datetime.strptime(date_part, '%Y%m%d')
                            print(f"      ID {id_val}: {date_obj.strftime('%Y-%m-%d')} âœ…")
                            date_success += 1
                        except ValueError:
                            print(f"      ID {id_val}: ç„¡æ³•è§£ææ—¥æœŸ âŒ")
                    
                    if date_success >= 2:
                        print(f"\n   âœ… çµè«–: ID åŒ…å«æ—¥æœŸ (å‰ 8 ç¢¼ = YYYYMMDD)")
                        print(f"   âœ… å¯ä½¿ç”¨ Turbo Mode")
                        self.report['turbo_mode'] = True
                    else:
                        print(f"\n   âŒ ID ä¸åŒ…å«æ—¥æœŸ")
                        self.report['turbo_mode'] = False
                else:
                    print(f"   âš ï¸  ID é•·åº¦ä¸è¶³ 8 ä½ï¼Œç„¡æ³•æå–æ—¥æœŸ")
                    print(f"   âŒ ç„¡æ³•ä½¿ç”¨ Turbo Mode")
                    self.report['turbo_mode'] = False
            else:
                print(f"   âš ï¸  ID é•·åº¦ä¸ä¸€è‡´: {set(id_lengths)}")
                self.report['turbo_mode'] = False
    
    async def _test_defenses(self):
        """æ¸¬è©¦é˜²ç«ç‰†ï¼ˆåŠ å¼·ç‰ˆï¼šå«å…§å®¹é©—è­‰ï¼‰"""
        print("\nã€æ­¥é©Ÿ 5/5ã€‘æ¸¬è©¦é˜²ç«ç‰†...")
        print("-" * 70)
        
        if not self.report['sample_articles']:
            print("âš ï¸  æ²’æœ‰ç¯„ä¾‹æ–‡ç« ï¼Œç„¡æ³•æ¸¬è©¦")
            return
        
        # âœ¨ ä½¿ç”¨æ¸…ç†å¾Œçš„ URL
        test_url = self.report['sample_articles'][0]
        print(f"æ¸¬è©¦ URL: {test_url}")
        print()
        
        results = {}
        successful_html = None
        successful_method = None
        
        # æ¸¬è©¦ 1: aiohttp
        print("ã€æ¸¬è©¦ 1ã€‘aiohttp")
        if AIOHTTP_AVAILABLE:
            success, status, time_ms, html = await self._test_aiohttp_with_content(test_url)
            results['aiohttp'] = {
                'success': success,
                'status': status,
                'time_ms': time_ms,
                'content_length': len(html) if html else 0
            }
            
            if success:
                print(f"   âœ… æˆåŠŸ | HTTP {status} | {time_ms:.0f}ms | {len(html)} bytes")
                successful_html = html
                successful_method = 'aiohttp'
            else:
                print(f"   âŒ å¤±æ•— | HTTP {status}")
        else:
            print("   âš ï¸  æœªå®‰è£")
            results['aiohttp'] = {'success': False, 'status': 'NOT_INSTALLED'}
        
        # æ¸¬è©¦ 2: curl_cffi
        print("\nã€æ¸¬è©¦ 2ã€‘curl_cffi (æ¨¡æ“¬ Chrome 120)")
        if CURL_CFFI_AVAILABLE:
            success, status, time_ms, html = await self._test_curl_cffi_with_content(test_url)
            results['curl_cffi'] = {
                'success': success,
                'status': status,
                'time_ms': time_ms,
                'content_length': len(html) if html else 0
            }
            
            if success:
                print(f"   âœ… æˆåŠŸ | HTTP {status} | {time_ms:.0f}ms | {len(html)} bytes")
                if not successful_html:
                    successful_html = html
                    successful_method = 'curl_cffi'
            else:
                print(f"   âŒ å¤±æ•— | HTTP {status}")
        else:
            print("   âš ï¸  æœªå®‰è£ (pip install curl_cffi)")
            results['curl_cffi'] = {'success': False, 'status': 'NOT_INSTALLED'}
        
        # âœ¨ å…§å®¹é©—è­‰
        if successful_html:
            print("\n" + "="*70)
            print("ğŸ“„ å…§å®¹é©—è­‰")
            print("="*70)
            
            validation_result = self._validate_content(successful_html)
            self.report['content_validation'] = validation_result
            
            print(f"\nä½¿ç”¨æ–¹æ³•: {successful_method}")
            print(f"HTML é•·åº¦: {len(successful_html)} bytes")
            
            # é¡¯ç¤º HTML å‰ 500 å­—å…ƒ
            print(f"\nã€HTML å‰ 500 å­—å…ƒã€‘")
            print("-" * 70)
            preview = successful_html[:500]
            print(preview)
            if len(successful_html) > 500:
                print("...")
            
            # é¡¯ç¤ºæå–çµæœ
            print(f"\nã€æå–æ¸¬è©¦ã€‘")
            print("-" * 70)
            
            if validation_result['title']:
                print(f"âœ… æ¨™é¡Œ: {validation_result['title']}")
            else:
                print(f"âŒ ç„¡æ³•æå–æ¨™é¡Œ")
            
            if validation_result['content_preview']:
                print(f"\nâœ… å…§æ–‡é è¦½ (å‰ 200 å­—):")
                print(f"   {validation_result['content_preview']}")
            else:
                print(f"\nâŒ ç„¡æ³•æå–å…§æ–‡")
            
            if validation_result['date']:
                print(f"\nâœ… ç™¼å¸ƒæ—¥æœŸ: {validation_result['date']}")
            
            # éŒ¯èª¤é é¢æª¢æŸ¥
            print(f"\nã€éŒ¯èª¤é é¢æª¢æŸ¥ã€‘")
            print("-" * 70)
            
            if validation_result['is_error_page']:
                print(f"âŒ é€™æ˜¯éŒ¯èª¤é é¢ï¼")
                print(f"   åŸå› : {validation_result['error_reason']}")
            else:
                print(f"âœ… ä¸æ˜¯éŒ¯èª¤é é¢")
        
        # çµè«–
        print("\n" + "="*70)
        print("ã€çµè«–ã€‘")
        print("="*70)
        
        if results.get('aiohttp', {}).get('success'):
            print("âœ… å»ºè­°ä½¿ç”¨: AIOHTTP (ç„¡éœ€ç‰¹æ®Šè™•ç†)")
            recommendation = "aiohttp"
        elif results.get('curl_cffi', {}).get('success'):
            print("âš ï¸  å»ºè­°ä½¿ç”¨: CURL_CFFI (aiohttp è¢«æ“‹)")
            recommendation = "curl_cffi"
        else:
            print("âŒ å…©ç¨®æ–¹æ³•éƒ½å¤±æ•—ï¼Œéœ€è¦é€²ä¸€æ­¥èª¿æŸ¥")
            recommendation = "unknown"
        
        self.report['defense_test'] = {
            'test_url': test_url,
            'results': results,
            'recommendation': recommendation
        }
    
    def _validate_content(self, html: str) -> Dict:
        """é©—è­‰æŠ“å–åˆ°çš„å…§å®¹"""
        soup = BeautifulSoup(html, 'lxml')
        
        result = {
            'title': None,
            'content_preview': None,
            'date': None,
            'is_error_page': False,
            'error_reason': None
        }
        
        # 1. æå–æ¨™é¡Œ
        title_tags = [
            soup.find('h1'),
            soup.find('title'),
            soup.find('meta', {'property': 'og:title'}),
        ]
        
        for tag in title_tags:
            if tag:
                if tag.name == 'meta':
                    result['title'] = tag.get('content', '').strip()
                else:
                    result['title'] = tag.get_text(strip=True)
                
                if result['title']:
                    break
        
        # 2. æå–å…§æ–‡
        content_selectors = [
            'article',
            '.article-content',
            '.content',
            'div[itemprop="articleBody"]',
            '.post-content',
        ]
        
        for selector in content_selectors:
            content_tag = soup.select_one(selector)
            if content_tag:
                text = content_tag.get_text(strip=True)
                if len(text) > 50:
                    result['content_preview'] = text[:200]
                    break
        
        if not result['content_preview']:
            paragraphs = soup.find_all('p')
            all_text = ' '.join([p.get_text(strip=True) for p in paragraphs])
            if len(all_text) > 50:
                result['content_preview'] = all_text[:200]
        
        # 3. æå–æ—¥æœŸ
        date_selectors = [
            'time',
            '.date',
            '.publish-date',
            'meta[property="article:published_time"]',
            'span[itemprop="datePublished"]',
        ]
        
        for selector in date_selectors:
            date_tag = soup.select_one(selector)
            if date_tag:
                if date_tag.name == 'meta':
                    result['date'] = date_tag.get('content', '').strip()
                else:
                    result['date'] = date_tag.get_text(strip=True)
                
                if result['date']:
                    break
        
        # 4. æª¢æŸ¥æ˜¯å¦ç‚ºéŒ¯èª¤é é¢
        error_keywords = [
            '404',
            'not found',
            'æ‰¾ä¸åˆ°',
            'é é¢ä¸å­˜åœ¨',
            'error',
            'éŒ¯èª¤',
            'ç„¡æ³•æ‰¾åˆ°',
        ]
        
        page_text = soup.get_text().lower()
        
        for keyword in error_keywords:
            if keyword in page_text and len(page_text) < 5000:
                result['is_error_page'] = True
                result['error_reason'] = f"åŒ…å«éŒ¯èª¤é—œéµå­—: {keyword}"
                break
        
        if len(html) < 1000 and not result['content_preview']:
            result['is_error_page'] = True
            result['error_reason'] = "å…§å®¹å¤ªçŸ­ä¸”ç„¡æ³•æå–æ–‡ç« å…§å®¹"
        
        return result
    
    # ==================== éšæ®µ 2ï¼šç¼ºæ¼è£œå…… ====================
    
    async def fill_gaps(self):
        """è£œå……ç¼ºæ¼è³‡è¨Š"""
        print("\n" + "="*70)
        print("ğŸ”§ éšæ®µ 3ï¼šç¼ºæ¼è£œå……")
        print("="*70)
        
        gaps_found = False
        
        if not self.report['list_page']:
            print("\nâš ï¸  æœªæ‰¾åˆ°åˆ—è¡¨é ")
            print("è«‹æä¾›åˆ—è¡¨é  URLï¼ˆå³æ™‚æ–°èã€æœ€æ–°æ–°èç­‰ï¼‰")
            print("ç¯„ä¾‹: https://www.cna.com.tw/list/aall.aspx")
            
            list_url = input("åˆ—è¡¨é  URL: ").strip()
            
            if list_url:
                if await self._validate_list_page(list_url):
                    self.report['list_page'] = list_url
                    print(f"âœ… åˆ—è¡¨é å·²è¨­å®š: {list_url}")
                    
                    await self._extract_sample_articles()
                    await self._analyze_url_pattern()
                else:
                    print("âš ï¸  åˆ—è¡¨é å¯èƒ½ç„¡æ•ˆ")
            
            gaps_found = True
        
        if not self.report['sample_articles']:
            print("\nâš ï¸  æœªæ‰¾åˆ°ç¯„ä¾‹æ–‡ç« ")
            print("è«‹æä¾› 3-5 å€‹ç¢ºå®šæœ‰å…§å®¹çš„æ–‡ç«  URL")
            print("ç¯„ä¾‹: https://www.cna.com.tw/news/aipl/202412290037.aspx")
            print("è¼¸å…¥å®Œæˆå¾Œï¼Œç›´æ¥æŒ‰ Enter çµæŸ")
            print()
            
            sample_urls = []
            for i in range(5):
                url = input(f"ç¯„ä¾‹ {i+1}: ").strip()
                if not url:
                    if i >= 3:
                        break
                    else:
                        print(f"   âš ï¸  è‡³å°‘éœ€è¦ 3 å€‹ç¯„ä¾‹")
                        continue
                sample_urls.append(url)
            
            if sample_urls:
                self.report['sample_articles'] = sample_urls
                print(f"âœ… å·²åŠ å…¥ {len(sample_urls)} å€‹ç¯„ä¾‹")
                
                await self._analyze_url_pattern()
                await self._test_defenses()
            
            gaps_found = True
        
        if self.report['subdomains']:
            print(f"\nâš ï¸  ç™¼ç¾ {len(self.report['subdomains'])} å€‹å­ç¶²åŸŸ:")
            for subdomain in self.report['subdomains'][:5]:
                print(f"   - {subdomain}")
            
            print("\næ˜¯å¦éœ€è¦åµæŸ¥å­ç¶²åŸŸï¼Ÿ(y/n)")
            answer = input("é¸æ“‡: ").strip().lower()
            
            if answer == 'y':
                print("âš ï¸  å­ç¶²åŸŸåµæŸ¥åŠŸèƒ½å°šæœªå¯¦ä½œ")
                print("æç¤º: å¯ä»¥æ‰‹å‹•å°‡å­ç¶²åŸŸç•¶ä½œæ–°çš„é¦–é  URL é‡æ–°åŸ·è¡ŒåµæŸ¥")
            
            gaps_found = True
        
        if not gaps_found:
            print("\nâœ… ç„¡éœ€è£œå……ï¼Œæ‰€æœ‰è³‡è¨Šå·²å®Œæ•´")
    
    # ==================== è¼”åŠ©æ–¹æ³• ====================
    
    async def _fetch_html(self, url: str) -> Optional[str]:
        """æŠ“å– HTMLï¼ˆå„ªå…ˆä½¿ç”¨ curl_cffiï¼‰"""
        if CURL_CFFI_AVAILABLE:
            try:
                async with AsyncSession() as session:
                    response = await session.get(
                        url,
                        headers=DEFAULT_HEADERS,
                        timeout=10,
                        impersonate="chrome120"
                    )
                    
                    if response.status_code == 200:
                        return response.text
            except Exception:
                pass
        
        if AIOHTTP_AVAILABLE:
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(
                        url,
                        headers=DEFAULT_HEADERS,
                        timeout=aiohttp.ClientTimeout(total=10),
                        ssl=False
                    ) as response:
                        if response.status == 200:
                            return await response.text()
            except Exception:
                pass
        
        return None
    
    def _find_categories(self, soup: BeautifulSoup) -> Dict:
        """å¾é¦–é æ‰¾åˆ†é¡"""
        categories = {}
        
        nav_links = (
            soup.select('nav a[href]') or
            soup.select('header a[href]') or
            soup.select('a[href*="/list/"]') or
            soup.select('a[href*="/category/"]')
        )
        
        for link in nav_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            if not text or len(text) > 20:
                continue
            
            match = re.search(r'/(?:list|category)/([a-z0-9]+)', href)
            if match:
                code = match.group(1)
                
                # ä½¿ç”¨ _normalize_url è™•ç†ç›¸å°è·¯å¾‘
                href = self._normalize_url(href, self.home_url)
                
                categories[code] = {
                    'name': text,
                    'url': href
                }
        
        return categories
    
    def _find_subdomains(self, soup: BeautifulSoup) -> set:
        """æ‰¾å­ç¶²åŸŸ"""
        subdomains = set()
        
        for link in soup.find_all('a', href=True):
            href = link['href']
            match = re.search(r'https?://([^/]+)', href)
            if match:
                subdomain = match.group(1)
                
                if (self.base_domain in subdomain and 
                    subdomain != self.base_domain and 
                    subdomain != f'www.{self.base_domain}'):
                    subdomains.add(subdomain)
        
        return subdomains
    
    async def _validate_list_page(self, url: str) -> bool:
        """é©—è­‰åˆ—è¡¨é æ˜¯å¦æœ‰æ•ˆï¼ˆæ”¹é€²ç‰ˆï¼šæ”¯æ´ Query Stringï¼‰"""
        html = await self._fetch_html(url)
        
        if not html:
            return False
        
        soup = BeautifulSoup(html, 'lxml')
        
        article_links = soup.find_all('a', href=True)
        
        # è¨ˆç®—æœ‰æ•ˆæ–‡ç« é€£çµæ•¸é‡
        article_count = 0
        
        for link in article_links:
            href = link['href']
            
            # æª¢æŸ¥è·¯å¾‘æ ¼å¼
            if any(pattern in href for pattern in ['/news/', '/article/', '.html']):
                article_count += 1
            # æª¢æŸ¥ Query String æ ¼å¼
            elif self._has_article_id(href):
                article_count += 1
            
            if article_count >= 5:
                return True
        
        return False
    
    def _extract_url_pattern(self, url: str) -> Optional[Dict]:
        """æå– URL çµæ§‹ï¼ˆæ”¹é€²ç‰ˆï¼šæ”¯æ´ Query Stringï¼‰"""
        # å…ˆå˜—è©¦ Query String æ ¼å¼
        parsed = urlparse(url)
        params = parse_qs(parsed.query)
        
        # æª¢æŸ¥å¸¸è¦‹çš„ ID åƒæ•¸
        id_params = ['news_id', 'id', 'article_id', 'post_id', 'nid', 'aid']
        
        for param in id_params:
            if param in params:
                id_value = params[param][0]
                
                # æ§‹å»ºæ¨¡å¼æè¿°
                pattern_desc = f"?{param}={{ID}}"
                
                # æå–å…¶ä»–åƒæ•¸
                other_params = {k: v[0] for k, v in params.items() if k != param}
                
                return {
                    'pattern': pattern_desc,
                    'id': id_value,
                    'params': other_params,
                    'url': url,
                    'type': 'query_string'
                }
        
        # å¦‚æœä¸æ˜¯ Query Stringï¼Œå˜—è©¦è·¯å¾‘æ ¼å¼
        path_patterns = [
            (r'/news/([a-z]+)/(\d+)', 'category+id'),
            (r'/article/(\d+)', 'id_only'),
            (r'/(\d+)\.html', 'id_only'),
            (r'/([a-z]+)/(\d+)', 'category+id'),
        ]
        
        for pattern, pattern_type in path_patterns:
            match = re.search(pattern, url)
            if match:
                groups = match.groups()
                
                if pattern_type == 'category+id':
                    return {
                        'pattern': pattern,
                        'category': groups[0],
                        'id': groups[1],
                        'url': url,
                        'type': 'path'
                    }
                else:
                    return {
                        'pattern': pattern,
                        'id': groups[0],
                        'url': url,
                        'type': 'path'
                    }
        
        return None
    
    async def _test_aiohttp_with_content(self, url: str) -> Tuple[bool, int, float, Optional[str]]:
        """æ¸¬è©¦ aiohttpï¼ˆè¿”å› HTMLï¼‰"""
        import time
        start = time.time()
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    url,
                    headers=DEFAULT_HEADERS,
                    timeout=aiohttp.ClientTimeout(total=10),
                    ssl=False
                ) as response:
                    elapsed = (time.time() - start) * 1000
                    
                    if response.status == 200:
                        html = await response.text()
                        return (len(html) > 1000, response.status, elapsed, html)
                    else:
                        return (False, response.status, elapsed, None)
        except Exception:
            elapsed = (time.time() - start) * 1000
            return (False, 0, elapsed, None)
    
    async def _test_curl_cffi_with_content(self, url: str) -> Tuple[bool, int, float, Optional[str]]:
        """æ¸¬è©¦ curl_cffiï¼ˆè¿”å› HTMLï¼‰"""
        import time
        start = time.time()
        
        try:
            async with AsyncSession() as session:
                response = await session.get(
                    url,
                    headers=DEFAULT_HEADERS,
                    timeout=10,
                    impersonate="chrome120"
                )
                
                elapsed = (time.time() - start) * 1000
                
                if response.status_code == 200:
                    html = response.text
                    return (len(html) > 1000, response.status_code, elapsed, html)
                else:
                    return (False, response.status_code, elapsed, None)
        except Exception:
            elapsed = (time.time() - start) * 1000
            return (False, 0, elapsed, None)
    
    # ==================== ç”¢å‡ºå ±å‘Š ====================
    
    def generate_report(self):
        """ç”¢å‡ºæœ€çµ‚å ±å‘Š"""
        print("\n" + "="*70)
        print("ğŸ“‹ æœ€çµ‚åµæŸ¥å ±å‘Š")
        print("="*70)
        
        print("\nã€åŸºæœ¬è³‡è¨Šã€‘")
        print("-" * 70)
        print(f"é¦–é : {self.report['home_url']}")
        print(f"ä¸»ç¶²åŸŸ: {self.report['base_domain']}")
        
        print("\nã€ä»»å‹™ Aï¼šåœ°å½¢åˆ†æã€‘")
        print("-" * 70)
        categories = self.report['categories']
        print(f"åˆ†é¡æ•¸é‡: {len(categories)}")
        if categories:
            print("ä¸»è¦åˆ†é¡:")
            for code, info in list(categories.items())[:5]:
                print(f"  - {info['name']:12s} ({code})")
        
        subdomains = self.report['subdomains']
        print(f"å­ç¶²åŸŸ: {'æœ‰ (' + str(len(subdomains)) + ' å€‹)' if subdomains else 'ç„¡'}")
        
        print("\nã€ä»»å‹™ Bï¼šæ°´æºå®šä½ã€‘")
        print("-" * 70)
        list_page = self.report['list_page']
        if list_page:
            print(f"âœ… åˆ—è¡¨é : {list_page}")
        else:
            print("âŒ æœªæ‰¾åˆ°åˆ—è¡¨é ")
        
        print("\nã€ä»»å‹™ Cï¼šæš—è™Ÿç ´è§£ã€‘")
        print("-" * 70)
        sample_count = len(self.report['sample_articles'])
        print(f"ç¯„ä¾‹æ•¸é‡: {sample_count}")
        
        # é¡¯ç¤ºç¯„ä¾‹ URL
        if sample_count > 0:
            display_count = min(5, sample_count)
            print(f"\nç¯„ä¾‹ URL (é¡¯ç¤ºå‰ {display_count} å€‹):")
            for i, url in enumerate(self.report['sample_articles'][:display_count], 1):
                print(f"  {i}. {url}")
            
            if sample_count > 5:
                print(f"  ... é‚„æœ‰ {sample_count - 5} å€‹")
        
        if self.report['url_pattern']:
            print(f"\nURL çµæ§‹: {self.report['url_pattern']}")
        else:
            print("\nURL çµæ§‹: æœªè§£æ")
        
        turbo = self.report['turbo_mode']
        print(f"Turbo Mode: {'âœ… å¯ç”¨' if turbo else 'âŒ ä¸å¯ç”¨'}")
        
        print("\nã€ä»»å‹™ Dï¼šé˜²ç¦¦æ¸¬è©¦ã€‘")
        print("-" * 70)
        defense = self.report['defense_test']
        
        if defense:
            results = defense.get('results', {})
            for method, result in results.items():
                status = "âœ… æˆåŠŸ" if result.get('success') else "âŒ å¤±æ•—"
                http_status = result.get('status', 'N/A')
                print(f"{method:12s}: {status} (HTTP {http_status})")
            
            recommendation = defense.get('recommendation', 'unknown')
            print(f"\nğŸ¯ å»ºè­°ä½¿ç”¨: {recommendation.upper()}")
        else:
            print("âš ï¸  æœªåŸ·è¡Œæ¸¬è©¦")
        
        # å…§å®¹é©—è­‰çµæœ
        if self.report.get('content_validation'):
            print("\nã€å…§å®¹é©—è­‰ã€‘")
            print("-" * 70)
            validation = self.report['content_validation']
            
            if validation['title']:
                print(f"âœ… å¯æå–æ¨™é¡Œ")
            else:
                print(f"âŒ ç„¡æ³•æå–æ¨™é¡Œ")
            
            if validation['content_preview']:
                print(f"âœ… å¯æå–å…§æ–‡")
            else:
                print(f"âŒ ç„¡æ³•æå–å…§æ–‡")
            
            if validation['is_error_page']:
                print(f"âŒ å…§å®¹é©—è­‰å¤±æ•—: {validation['error_reason']}")
            else:
                print(f"âœ… å…§å®¹é©—è­‰é€šé")
        
        # å„²å­˜ JSON
        print("\n" + "="*70)
        report_file = "temp/probe_report.json"
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(self.report, f, ensure_ascii=False, indent=2)
            print(f"âœ… å ±å‘Šå·²å„²å­˜: {report_file}")
        except Exception as e:
            print(f"âš ï¸  ç„¡æ³•å„²å­˜å ±å‘Š: {e}")
        
        print("="*70)
        print("åµæŸ¥å®Œæˆï¼")
        print("="*70)


# ==================== ä¸»ç¨‹å¼ ====================

async def main():
    """åŸ·è¡Œè‡ªå‹•åŒ–åµæŸ¥"""
    print("ğŸ•µï¸  è‡ªå‹•åŒ–ç¶²ç«™åµæŸ¥å·¥å…·ï¼ˆæ”¹é€²ç‰ˆ v3ï¼‰")
    print("Phase 0 - æˆ°å ´åµæŸ¥ (Reconnaissance)")
    print()
    print("æ”¹é€²:")
    print("  âœ… æ”¯æ´ Query String æ ¼å¼ (?news_id=123)")
    print("  âœ… éæ¿¾åˆ—è¡¨é ï¼Œåªä¿ç•™æ–‡ç« é ")
    print("  âœ… å¢åŠ  URL æ¸…ç†é‚è¼¯ï¼ˆè™•ç†ç›¸å°è·¯å¾‘ï¼‰")
    print("  âœ… æŠ“å–æ‰€æœ‰æ–‡ç« é€£çµï¼ˆä¸é™åˆ¶æ•¸é‡ï¼‰")
    print()
    print("âš ï¸  é€™æ˜¯è‡¨æ™‚åµæŸ¥è…³æœ¬ï¼Œä¸æœƒä¿®æ”¹ä»»ä½•æ­£å¼ç¨‹å¼ç¢¼")
    print()
    
    if not CURL_CFFI_AVAILABLE and not AIOHTTP_AVAILABLE:
        print("âŒ éŒ¯èª¤: è‡³å°‘éœ€è¦å®‰è£ aiohttp æˆ– curl_cffi")
        print("   pip install aiohttp")
        print("   pip install curl_cffi")
        return
    
    if not CURL_CFFI_AVAILABLE:
        print("âš ï¸  è­¦å‘Š: curl_cffi æœªå®‰è£ï¼Œéƒ¨åˆ†ç¶²ç«™å¯èƒ½ç„¡æ³•æŠ“å–")
        print("   å»ºè­°å®‰è£: pip install curl_cffi")
        print()
    
    print("="*70)
    print("ğŸ“ éšæ®µ 1ï¼šè¼¸å…¥é¦–é  URL")
    print("="*70)
    print()
    print("è«‹è¼¸å…¥è¦åµæŸ¥çš„ç¶²ç«™é¦–é  URL")
    print("ç¯„ä¾‹: https://www.cna.com.tw")
    print()
    
    home_url = input("é¦–é  URL: ").strip()
    
    if not home_url:
        print("âŒ é¦–é  URL ä¸å¯ç‚ºç©º")
        return
    
    if not home_url.startswith('http'):
        print("âŒ URL å¿…é ˆä»¥ http:// æˆ– https:// é–‹é ­")
        return
    
    print(f"âœ… é¦–é : {home_url}")
    
    scout = AutoScout(home_url)
    
    await scout.auto_reconnaissance()
    await scout.fill_gaps()
    scout.generate_report()


if __name__ == "__main__":
    asyncio.run(main())
