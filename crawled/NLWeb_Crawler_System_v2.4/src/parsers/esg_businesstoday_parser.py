"""
ä»Šå‘¨åˆŠ ESG è§£æå™¨ (AJAX ç­–ç•¥)
=================================

âœ… FIX #WORK-ORDER-926: å¯¦ä½œ List-Based Date Searchï¼ˆåˆ—è¡¨æƒææ—¥æœŸæœå°‹ï¼‰
âœ… FIX #DATE-NAV-001: ä¿®æ­£ ID ä½æ•¸å•é¡Œï¼ˆ14ç¢¼ â†’ 12ç¢¼ï¼‰
âœ… FIX #WORK-ORDER-925: è£œä¸Š get_date æ–¹æ³•ï¼ˆå¾ ID æå–æ—¥æœŸï¼‰

ç­–ç•¥ï¼š
1. åˆ©ç”¨ AJAX æ¥å£ (/catalog/{cat_id}/list/page/{page}/ajax) é€²è¡Œåˆ—è¡¨çˆ¬å–
2. æ”¯æ´ 5 å¤§åˆ†é¡çš„å¤šé æŠ“å–
3. åš´æ ¼éµå®ˆ Schema.org NewsArticle æ ¼å¼
4. æ•´åˆ TextProcessor é€²è¡Œæ–‡æœ¬æ¸…æ´—
5. å¾æ–‡ç«  ID ç›´æ¥æå–æ—¥æœŸï¼ˆç„¡éœ€ç¶²è·¯è«‹æ±‚ï¼‰
6. è‡ªå‹•ä¿®æ­£ DateNavigator å‚³å…¥çš„ 14 ç¢¼ ID
7. æ”¯æ´æ—¥æœŸç¯„åœæœå°‹ï¼ˆåˆ—è¡¨æƒæç­–ç•¥ï¼‰

Author: Agent B
Date: 2026-01-01
Priority: P0 (Critical)
"""

from typing import Optional, Dict, Any, List
from datetime import datetime
import re
import time
from bs4 import BeautifulSoup
from curl_cffi import requests
from src.core.interfaces import BaseParser, SessionType
from src.utils.text_processor import TextProcessor


class EsgBusinessTodayParser(BaseParser):
    """
    ä»Šå‘¨åˆŠ ESG è§£æå™¨
    
    æŠ€è¡“ç‰¹é»ï¼š
    - ä½¿ç”¨ AJAX æ¥å£ç¹éæ»¾å‹•è¼‰å…¥é™åˆ¶
    - curl_cffi å½è£è«‹æ±‚ç¹é WAF
    - åš´æ ¼ Schema.org NewsArticle æ ¼å¼
    - æ™ºèƒ½æ–‡æœ¬æ¸…æ´—èˆ‡æ‘˜è¦æå–
    - å¾æ–‡ç«  ID ç›´æ¥æå–æ—¥æœŸï¼ˆé«˜æ•ˆèƒ½ï¼‰
    - è‡ªå‹•ä¿®æ­£ ID ä½æ•¸ï¼ˆ14ç¢¼ â†’ 12ç¢¼ï¼‰
    - åˆ—è¡¨æƒææ—¥æœŸæœå°‹ï¼ˆList-Based Date Searchï¼‰
    """
    
    BASE_URL = "https://esg.businesstoday.com.tw"
    
    # ä¸»è¦åˆ†é¡ ID (æ ¹æ“šå®˜æ–¹å°èˆªåˆ—)
    CATEGORIES = {
        180686: "å…¨éƒ¨",
        180687: "Eæ°¸çºŒç’°å¢ƒ",
        180688: "Sç¤¾æœƒè²¬ä»»",
        180689: "Gå…¬å¸æ²»ç†",
        190807: "ESGå¿«è¨Š"
    }

    preferred_session_type = SessionType.CURL_CFFI

    def __init__(self, count: Optional[int] = None, **kwargs):
        super().__init__()
        self.count = count or 50
        self._discovered_ids = []
        self._id_to_url_map = {}  # å„²å­˜ ID èˆ‡å®Œæ•´ URL çš„å°æ‡‰

    @property
    def source_name(self) -> str:
        return "esg_businesstoday"

    async def get_latest_id(self) -> Optional[int]:
        """å–å¾—æœ€æ–°æ–‡ç«  ID"""
        ids = self.get_discovered_ids()
        return ids[0] if ids else None

    async def get_date(self, article_id: int) -> Optional[datetime]:
        """
        å¾æ–‡ç«  ID æå–ç™¼å¸ƒæ—¥æœŸ
        
        ä»Šå‘¨åˆŠ ESG çš„æ–‡ç«  ID æ ¼å¼ï¼šYYYYMMDDXXXX (12ç¢¼)
        - å‰ 8 ä½ï¼šæ—¥æœŸ (YYYYMMDD)
        - å¾Œ 4 ä½ï¼šæµæ°´è™Ÿ
        
        âš ï¸ é‡è¦ï¼šDateNavigator å¯èƒ½å‚³å…¥ 14 ç¢¼ IDï¼Œéœ€è‡ªå‹•ä¿®æ­£
        
        ç¯„ä¾‹ï¼š
        - 202512310016 (12ç¢¼) â†’ 2025-12-31 âœ…
        - 20251231001600 (14ç¢¼) â†’ è‡ªå‹•ä¿®æ­£ç‚º 202512310016 â†’ 2025-12-31 âœ…
        
        Args:
            article_id: æ–‡ç«  ID (12ç¢¼æˆ–14ç¢¼)
            
        Returns:
            datetime ç‰©ä»¶ï¼Œè‹¥è§£æå¤±æ•—å‰‡è¿”å› None
        """
        try:
            # âœ… [FIX] ä½æ•¸ä¿®æ­£ï¼šè™•ç† DateNavigator å‚³å…¥çš„ 14 ç¢¼ ID
            id_str = str(article_id)
            
            if len(id_str) == 14:
                # å»æ‰æœ€å¾Œå…©ç¢¼ï¼Œè®Šå› 12 ç¢¼
                id_str = id_str[:-2]
                self._logger.debug(f"ID ä½æ•¸ä¿®æ­£: {article_id} (14ç¢¼) â†’ {id_str} (12ç¢¼)")
            
            if len(id_str) < 8:
                self._logger.warning(f"Invalid article ID format: {article_id}")
                return None
            
            # æå–æ—¥æœŸéƒ¨åˆ† (YYYYMMDD)
            date_str = id_str[:8]
            
            # è§£æç‚º datetime
            date_obj = datetime.strptime(date_str, '%Y%m%d')
            
            return date_obj
            
        except ValueError as e:
            self._logger.error(f"Failed to parse date from ID {article_id}: {e}")
            return None
        except Exception as e:
            self._logger.error(f"Unexpected error in get_date for ID {article_id}: {e}")
            return None

    def _parse_date_from_id(self, article_id: int) -> Optional[datetime]:
        """
        å…§éƒ¨è¼”åŠ©ï¼šç›´æ¥å¾ ID è§£ææ—¥æœŸï¼ˆåŒæ­¥æ–¹æ³•ï¼‰
        
        âœ… FIX #WORK-ORDER-926: æ–°å¢æ–¹æ³•
        
        Args:
            article_id: æ–‡ç«  ID (12ç¢¼æˆ–14ç¢¼)
            
        Returns:
            datetime ç‰©ä»¶ï¼Œè‹¥è§£æå¤±æ•—å‰‡è¿”å› None
        """
        try:
            id_str = str(article_id)
            
            # ä½æ•¸ä¿®æ­£
            if len(id_str) == 14:
                id_str = id_str[:-2]
            
            if len(id_str) >= 8:
                date_str = id_str[:8]
                return datetime.strptime(date_str, '%Y%m%d')
        except (ValueError, IndexError):
            pass
        return None

    def _fetch_ids_from_page(self, url: str) -> List[int]:
        """
        å…§éƒ¨è¼”åŠ©ï¼šæŠ“å–å–®é çš„æ‰€æœ‰ ID
        
        âœ… FIX #WORK-ORDER-926: æ–°å¢æ–¹æ³•
        
        Args:
            url: åˆ—è¡¨é  URL
            
        Returns:
            æ–‡ç«  ID åˆ—è¡¨
        """
        ids = []
        try:
            r = requests.get(
                url, 
                impersonate="chrome110", 
                timeout=10,
                headers={
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                    "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.8",
                }
            )
            
            if r.status_code == 200:
                soup = BeautifulSoup(r.text, 'lxml')
                links = soup.select('a.article__item, a.hover-area, a[href*="/post/"]')
                
                for link in links:
                    href = link.get('href', '')
                    match = re.search(r'/post/(\d+)', href)
                    if match:
                        article_id = int(match.group(1))
                        ids.append(article_id)
                        
                        # å„²å­˜å®Œæ•´ URL
                        if article_id not in self._id_to_url_map:
                            if href.startswith('/'):
                                full_url = self.BASE_URL + href
                            else:
                                full_url = href
                            self._id_to_url_map[article_id] = full_url
        except Exception as e:
            self._logger.debug(f"Error fetching page {url}: {e}")
        
        return ids

    def get_ids_by_date_range(self, start_date: datetime, end_date: datetime) -> List[int]:
        """
        [åˆ—è¡¨æƒæç­–ç•¥] æ ¹æ“šæ—¥æœŸç¯„åœæœå°‹æ–‡ç«  ID
        
        âœ… FIX #WORK-ORDER-926: æ ¸å¿ƒæ–¹æ³•
        
        é‚è¼¯ï¼š
        1. æƒææ‰€æœ‰åˆ†é¡åˆ—è¡¨
        2. ç”±æ–°åˆ°èˆŠæª¢æŸ¥æ–‡ç« æ—¥æœŸ
        3. ä¸€æ—¦é‡åˆ°æ¯” start_date é‚„èˆŠçš„æ–‡ç« ï¼Œå°±åœæ­¢è©²åˆ†é¡çš„æƒæ
        4. æ”¶é›†æ‰€æœ‰ç¬¦åˆç¯„åœçš„ ID
        
        Args:
            start_date: é–‹å§‹æ—¥æœŸ
            end_date: çµæŸæ—¥æœŸ
            
        Returns:
            ç¬¦åˆæ—¥æœŸç¯„åœçš„æ–‡ç«  ID åˆ—è¡¨ï¼ˆé™åºæ’åˆ—ï¼‰
        """
        found_ids = set()
        print(f"ğŸ” [ESG] Scanning lists for date range: {start_date.date()} ~ {end_date.date()}")
        
        for cat_id, cat_name in self.CATEGORIES.items():
            print(f"  ğŸ“‚ Scanning category: {cat_name} ({cat_id})...")
            page = 1
            stop_category = False
            
            while not stop_category:
                # æ§‹å»º URL (ç¬¬1é éœæ…‹ï¼Œç¬¬2é + AJAX)
                if page == 1:
                    url = f"{self.BASE_URL}/catalog/{cat_id}/"
                else:
                    url = f"{self.BASE_URL}/catalog/{cat_id}/list/page/{page}/ajax"
                
                # æŠ“å–ä¸¦è§£æ ID
                page_ids = self._fetch_ids_from_page(url)
                
                if not page_ids:
                    print(f"    - Page {page}: No articles found, stopping category.")
                    break  # æ²’è³‡æ–™äº†ï¼Œæ›ä¸‹ä¸€å€‹åˆ†é¡
                
                # æª¢æŸ¥é€™ä¸€é çš„æ‰€æœ‰ ID
                valid_count_in_page = 0
                for aid in page_ids:
                    # ä½¿ç”¨åŒæ­¥æ–¹æ³•è§£ææ—¥æœŸ
                    adate = self._parse_date_from_id(aid)
                    
                    if not adate:
                        continue
                    
                    if adate > end_date:
                        continue  # å¤ªæ–°ï¼Œç¹¼çºŒæ‰¾ä¸‹ä¸€ç¯‡
                    
                    if adate < start_date:
                        # ç™¼ç¾æ–‡ç« æ¯”é–‹å§‹æ—¥æœŸé‚„èˆŠï¼Œåœæ­¢è©²åˆ†é¡
                        stop_category = True
                        break
                    
                    # ç¬¦åˆç¯„åœ
                    found_ids.add(aid)
                    valid_count_in_page += 1
                
                print(f"    - Page {page}: Found {valid_count_in_page} matching articles.")
                
                if stop_category:
                    print(f"    â¹ï¸  Reached older articles. Stopping category {cat_name}.")
                    break
                
                page += 1
                time.sleep(0.5)  # ç¦®è²Œæ€§å»¶é²
                
                # å®‰å…¨é–¥ï¼šé¿å…ç„¡çª®è¿´åœˆ
                if page > 50:
                    print("    âš ï¸  Page limit reached.")
                    break
        
        sorted_ids = sorted(list(found_ids), reverse=True)
        print(f"âœ… [ESG] Total found: {len(sorted_ids)} unique articles in range.")
        return sorted_ids

    def get_discovered_ids(self) -> List[int]:
        """
        åˆ©ç”¨ AJAX æ¥å£æŠ“å–å¤šé æ–‡ç«  ID
        
        ç­–ç•¥ï¼š
        1. ç¬¬ 1 é ï¼šéœæ…‹ HTML (/catalog/{cat_id}/)
        2. ç¬¬ 2-N é ï¼šAJAX æ¥å£ (/catalog/{cat_id}/list/page/{page}/ajax)
        3. æ¯å€‹åˆ†é¡æŠ“å– 3 é  (ç´„ 50-60 ç¯‡æ–‡ç« )
        """
        found_ids = set()
        
        for cat_id, cat_name in self.CATEGORIES.items():
            print(f"\nğŸ“‚ æƒæåˆ†é¡: {cat_name} (ID: {cat_id})")
            
            # 1. æŠ“å–ç¬¬ 1 é  (éœæ…‹)
            url_p1 = f"{self.BASE_URL}/catalog/{cat_id}/"
            page_ids = self._fetch_ids_from_page(url_p1)
            found_ids.update(page_ids)
            print(f"  âœ… ç¬¬ 1 é ï¼šæ–°å¢ {len(page_ids)} ç¯‡")
            
            # 2. æŠ“å–ç¬¬ 2-3 é  (AJAX)
            for page in range(2, 4):
                url_ajax = f"{self.BASE_URL}/catalog/{cat_id}/list/page/{page}/ajax"
                page_ids = self._fetch_ids_from_page(url_ajax)
                found_ids.update(page_ids)
                print(f"  âœ… ç¬¬ {page} é ï¼šæ–°å¢ {len(page_ids)} ç¯‡")
                time.sleep(0.5)  # ç¦®è²Œæ€§å»¶é²
                
                # æå‰çµ‚æ­¢æ¢ä»¶
                if len(found_ids) >= self.count * 1.5:
                    print(f"âœ… å·²æ”¶é›†è¶³å¤ æ–‡ç«  ({len(found_ids)} ç¯‡)")
                    break
            
            time.sleep(1)  # åˆ†é¡é–“å»¶é²
        
        # è½‰ç‚ºåˆ—è¡¨ä¸¦æ’åº (æ–°åˆ°èˆŠ)
        sorted_ids = sorted(list(found_ids), reverse=True)
        self._discovered_ids = sorted_ids[:self.count]
        
        print(f"\nâœ… ç¸½å…±ç™¼ç¾ {len(sorted_ids)} ç¯‡æ–‡ç« ï¼Œä¿ç•™å‰ {len(self._discovered_ids)} ç¯‡")
        return self._discovered_ids

    def get_url(self, article_id: int) -> str:
        """
        æ§‹å»ºæ–‡ç«  URL (å«ä½æ•¸è‡ªå‹•ä¿®æ­£)
        
        âœ… [FIX #DATE-NAV-001] ä½æ•¸ä¿®æ­£é‚è¼¯
        """
        # 1. å„ªå…ˆä½¿ç”¨å¿«å–
        if article_id in self._id_to_url_map:
            return self._id_to_url_map[article_id]
            
        # 2. âœ… [FIX] ä½æ•¸ä¿®æ­£ï¼šè™•ç† DateNavigator å‚³å…¥çš„ 14 ç¢¼ ID
        id_str = str(article_id)
        if len(id_str) == 14:
            article_id = int(id_str[:-2])
            self._logger.debug(f"URL ä½æ•¸ä¿®æ­£: {id_str} (14ç¢¼) â†’ {article_id} (12ç¢¼)")
            
        # 3. å‚™ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ "å…¨éƒ¨" åˆ†é¡
        return f"{self.BASE_URL}/article/category/180686/post/{article_id}"

    async def parse(self, html: str, url: str) -> Optional[Dict[str, Any]]:
        """
        è§£æå–®ç¯‡æ–‡ç« 
        
        åš´æ ¼éµå®ˆ Schema.org NewsArticle æ ¼å¼
        æ‰€æœ‰æ¬„ä½å¿…é ˆæ˜¯ç´”å­—ä¸²æˆ–å­—ä¸²åˆ—è¡¨ï¼Œä¸å¾—ç‚ºç‰©ä»¶
        """
        try:
            soup = BeautifulSoup(html, 'lxml')
            
            # ========== 1. æ¨™é¡Œ (headline) ==========
            headline = None
            
            # æ–¹æ³• 1: h1 æ¨™ç±¤
            h1_tag = soup.select_one('div.content_top h1, h1')
            if h1_tag:
                headline = TextProcessor.clean_text(h1_tag.get_text())
            
            # æ–¹æ³• 2: og:title
            if not headline:
                og_title = soup.find('meta', property='og:title')
                if og_title:
                    headline = TextProcessor.clean_text(og_title.get('content', ''))
                    headline = headline.replace('ï¼ESGæ°¸çºŒå°ç£', '').strip()
            
            if not headline:
                print(f"âš ï¸ ç„¡æ³•æå–æ¨™é¡Œ: {url}")
                return None
            
            # ========== 2. å…§æ–‡ (articleBody) ==========
            article_body = ""
            
            # ä¸»è¦å…§æ–‡å€åŸŸ
            content_div = soup.select_one('div[itemprop="articleBody"]')
            if content_div:
                paragraphs = []
                for p in content_div.find_all(['p', 'h2', 'h3']):
                    text = TextProcessor.clean_text(p.get_text())
                    if text and len(text) > 10:
                        paragraphs.append(text)
                
                # âœ… FIX #WORK-ORDER-926: ç§»é™¤ max_length åƒæ•¸
                article_body = TextProcessor.smart_extract_summary(paragraphs)
            
            # è£œå……ï¼šæ‘˜è¦å€åŸŸ
            summary_div = soup.select_one('div.articlemark p')
            if summary_div:
                summary_text = TextProcessor.clean_text(summary_div.get_text())
                if summary_text and summary_text not in article_body:
                    article_body = summary_text + "\n\n" + article_body
            
            if not article_body:
                print(f"âš ï¸ ç„¡æ³•æå–å…§æ–‡: {url}")
                return None
            
            # ========== 3. ä½œè€… (author) ==========
            author = "ä»Šå‘¨åˆŠ"
            
            author_section = soup.select_one('div.author_left')
            if author_section:
                author_text = author_section.get_text()
                match = re.search(r'æ’°æ–‡ï¼š\s*(.+?)(?:\s|&nbsp;|åˆ†é¡ï¼š)', author_text)
                if match:
                    raw_author = match.group(1).strip()
                    author = TextProcessor.clean_author(raw_author)
            
            if author == "ä»Šå‘¨åˆŠ":
                meta_author = soup.find('meta', attrs={'name': 'author'})
                if meta_author:
                    author = TextProcessor.clean_author(meta_author.get('content', ''))
            
            if not isinstance(author, str):
                author = "ä»Šå‘¨åˆŠ"
            
            # ========== 4. ç™¼å¸ƒæ—¥æœŸ (datePublished) ==========
            date_published = None
            
            if author_section:
                date_match = re.search(r'æ—¥æœŸï¼š(\d{4}-\d{2}-\d{2})', author_section.get_text())
                if date_match:
                    date_str = date_match.group(1)
                    try:
                        dt = datetime.strptime(date_str, '%Y-%m-%d')
                        date_published = dt.isoformat()
                    except:
                        pass
            
            if not date_published:
                url_match = re.search(r'/post/(\d{8})', url)
                if url_match:
                    date_str = url_match.group(1)
                    try:
                        dt = datetime.strptime(date_str, '%Y%m%d')
                        date_published = dt.isoformat()
                    except:
                        pass
            
            if not date_published:
                date_published = datetime.now().isoformat()
            
            # ========== 5. åˆ†é¡ (keywords) ==========
            keywords = []
            
            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
            if meta_keywords:
                kw_text = meta_keywords.get('content', '')
                keywords = [k.strip() for k in kw_text.split(',') if k.strip()]
            
            breadcrumb = soup.select_one('div.esg-breadcrumb')
            if breadcrumb:
                for link in breadcrumb.find_all('a'):
                    cat_name = TextProcessor.clean_text(link.get_text())
                    if cat_name and cat_name != 'é¦–é ' and cat_name not in keywords:
                        keywords.append(cat_name)
            
            keywords = [str(k) for k in keywords if k]
            
            # ========== 6. åœ–ç‰‡ä¾†æº ==========
            image_source = None
            if author_section:
                img_match = re.search(r'åœ–æª”ä¾†æºï¼š(.+?)(?:æ—¥æœŸï¼š|$)', author_section.get_text())
                if img_match:
                    image_source = TextProcessor.clean_text(img_match.group(1))
            
            # ========== 7. ä¸»åœ– URL ==========
            image_url = None
            main_img = soup.select_one('div.content_top img')
            if main_img:
                image_url = main_img.get('src', '')
                if image_url and not image_url.startswith('http'):
                    image_url = self.BASE_URL + image_url
            
            # ========== çµ„åˆçµæœ ==========
            result = {
                "@type": "NewsArticle",
                "headline": headline,
                "articleBody": article_body,
                "author": author,
                "publisher": "ä»Šå‘¨åˆŠ ESG",
                "datePublished": date_published,
                "inLanguage": "zh-TW",
                "url": url,
                "keywords": keywords,
            }
            
            if image_source:
                result["imageSource"] = image_source
            if image_url:
                result["imageUrl"] = image_url
            
            return result
            
        except Exception as e:
            print(f"âŒ è§£æéŒ¯èª¤ ({url}): {e}")
            import traceback
            traceback.print_exc()
            return None


# ========== æ¸¬è©¦ç¨‹å¼ç¢¼ ==========
if __name__ == "__main__":
    import asyncio
    from datetime import timedelta
    
    async def test_parser():
        """æ¸¬è©¦ EsgBusinessTodayParserï¼ˆå«æ—¥æœŸç¯„åœæœå°‹ï¼‰"""
        print("="*80)
        print("ğŸ§ª ä»Šå‘¨åˆŠ ESG Parser æ¸¬è©¦ï¼ˆåˆ—è¡¨æœå°‹ç‰ˆï¼‰")
        print("="*80)
        
        parser = EsgBusinessTodayParser(count=10)
        
        # æ¸¬è©¦ 1ï¼šæ—¥æœŸç¯„åœæœå°‹
        print("\nã€æ¸¬è©¦ 1ã€‘æ—¥æœŸç¯„åœæœå°‹ï¼ˆList-Based Date Searchï¼‰")
        print("-"*80)
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=7)  # æœ€è¿‘ 7 å¤©
        
        print(f"æœå°‹ç¯„åœ: {start_date.date()} ~ {end_date.date()}")
        
        ids = parser.get_ids_by_date_range(start_date, end_date)
        
        print(f"\nâœ… æ‰¾åˆ° {len(ids)} ç¯‡æ–‡ç« ")
        if ids:
            print(f"æœ€æ–° 5 ç¯‡ ID: {ids[:5]}")
            
            # é©—è­‰æ—¥æœŸ
            print("\né©—è­‰æ—¥æœŸï¼š")
            for article_id in ids[:5]:
                date = parser._parse_date_from_id(article_id)
                if date:
                    print(f"  ID {article_id} â†’ {date.strftime('%Y-%m-%d')}")
        
        # æ¸¬è©¦ 2ï¼šå–®ç¯‡è§£æ
        if ids:
            print("\nã€æ¸¬è©¦ 2ã€‘å–®ç¯‡æ–‡ç« è§£æ")
            print("-"*80)
            test_id = ids[0]
            test_url = parser.get_url(test_id)
            print(f"æ¸¬è©¦æ–‡ç« : {test_url}")
            
            r = requests.get(test_url, impersonate="chrome110", timeout=15)
            if r.status_code == 200:
                result = await parser.parse(r.text, test_url)
                
                if result:
                    print("\nâœ… è§£ææˆåŠŸï¼")
                    print(f"æ¨™é¡Œ: {result['headline']}")
                    print(f"ä½œè€…: {result['author']}")
                    print(f"æ—¥æœŸ: {result['datePublished']}")
                    print(f"åˆ†é¡: {', '.join(result['keywords'])}")
                    print(f"å…§æ–‡é•·åº¦: {len(result['articleBody'])} å­—")
                else:
                    print("âŒ è§£æå¤±æ•—")
            else:
                print(f"âŒ HTTP {r.status_code}")
        
        print("\n" + "="*80)
        print("âœ… æ¸¬è©¦å®Œæˆï¼ˆåˆ—è¡¨æœå°‹å·²é©—è­‰ï¼‰")
        print("="*80)
    
    asyncio.run(test_parser())
