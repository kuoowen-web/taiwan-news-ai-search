"""
einfo_parser.py - ç’°å¢ƒè³‡è¨Šä¸­å¿ƒè§£æå™¨

âœ… æ´¾å·¥å–® #922-B: Schema æ¨™æº–åŒ–
- å®Œå…¨é‡æ§‹è¼¸å‡ºæ ¼å¼
- ä½¿ç”¨æ™ºæ…§æ‘˜è¦
- æ–°å¢ keywords æ¬„ä½
- ç¬¦åˆæ¨™æº– Schema
"""

from typing import Optional, Dict, Any, List
from datetime import datetime
import re
from bs4 import BeautifulSoup
from curl_cffi import requests
from src.core.interfaces import BaseParser, SessionType
from src.utils.text_processor import TextProcessor


class EInfoParser(BaseParser):
    """ç’°å¢ƒè³‡è¨Šä¸­å¿ƒ (E-Info) Parser"""
    
    BASE_URL = "https://e-info.org.tw"
    CATEGORY_URLS = [
        "https://e-info.org.tw/taxonomy/term/258/all",
        "https://e-info.org.tw/taxonomy/term/266",
        "https://e-info.org.tw/taxonomy/term/35283/all"
    ]
    
    preferred_session_type = SessionType.CURL_CFFI

    def __init__(
        self, 
        count: Optional[int] = None,
        start_id: Optional[int] = None,
        target_date: Optional[datetime] = None,
        **kwargs
    ):
        super().__init__()
        self.count = count or 50
        self.start_id = start_id
        self.target_date = target_date
        self._discovered_ids: List[int] = []

    @property
    def source_name(self) -> str:
        return "einfo"
        
    async def get_latest_id(self) -> Optional[int]:
        """å‹•æ…‹ç²å–æœ€æ–° ID"""
        try:
            if self.start_id:
                latest_id = self.start_id
            else:
                latest_id = await self._fetch_latest_id_from_lists()
                if not latest_id:
                    print("âš ï¸  ç„¡æ³•åµæ¸¬æœ€æ–° IDï¼Œä½¿ç”¨é è¨­å€¼ 242797")
                    latest_id = 242797
            
            self._discovered_ids = list(range(
                latest_id,
                latest_id - self.count,
                -1
            ))
            
            if self._discovered_ids:
                print(f"âœ… åµæ¸¬åˆ°æœ€æ–° ID: {latest_id}")
                print(f"ğŸ“Š å°‡æŠ“å– {len(self._discovered_ids)} ç¯‡æ–‡ç«  (ID: {latest_id} â†’ {self._discovered_ids[-1]})")
                return self._discovered_ids[0]
            
            return None
            
        except Exception as e:
            print(f"âŒ get_latest_id éŒ¯èª¤: {e}")
            return None
    
    async def _fetch_latest_id_from_lists(self) -> Optional[int]:
        """å¾ä¸‰å€‹åˆ—è¡¨é æå–æœ€å¤§çš„ Node ID"""
        max_id = 0
        
        for url in self.CATEGORY_URLS:
            try:
                print(f"ğŸ” åµæŸ¥åˆ—è¡¨é : {url}")
                response = requests.get(
                    url, 
                    impersonate="chrome110", 
                    timeout=30
                )
                
                if response.status_code != 200:
                    continue
                
                soup = BeautifulSoup(response.text, 'lxml')
                node_links = soup.find_all('a', href=re.compile(r'/node/(\d+)'))
                
                for link in node_links:
                    href = link.get('href', '')
                    match = re.search(r'/node/(\d+)', href)
                    if match:
                        node_id = int(match.group(1))
                        max_id = max(max_id, node_id)
                
                print(f"   âœ“ æ‰¾åˆ° {len(node_links)} å€‹é€£çµï¼Œæœ€å¤§ ID: {max_id}")
                
            except Exception as e:
                print(f"âš ï¸  åˆ—è¡¨é æŠ“å–å¤±æ•— ({url}): {e}")
                continue
        
        return max_id if max_id > 0 else None
    
    def get_discovered_ids(self) -> List[int]:
        return self._discovered_ids
    
    def get_url(self, article_id: int) -> str:
        return f"{self.BASE_URL}/node/{article_id}"
    
    async def get_date(self, article_id: int) -> Optional[datetime]:
        """çµ¦ Navigator ç”¨ï¼ˆå›å‚³ datetime ç‰©ä»¶ï¼‰"""
        try:
            url = self.get_url(article_id)
            response = requests.get(
                url, 
                impersonate="chrome110", 
                timeout=30
            )
            
            if response.status_code != 200:
                return None
            
            soup = BeautifulSoup(response.text, 'lxml')
            date_str = self._extract_date(soup)
            if not date_str: 
                return None
            
            return self._parse_date(date_str)
            
        except Exception:
            return None
    
    async def parse(self, html: str, url: str) -> Optional[Dict[str, Any]]:
        """
        è§£æ HTML å…§å®¹
        
        âœ… Schema æ¨™æº–åŒ– (æ´¾å·¥å–® #922-B)
        """
        try:
            soup = BeautifulSoup(html, 'lxml')
            
            match = re.search(r'/node/(\d+)', url)
            article_id = int(match.group(1)) if match else 0

            title = self._extract_title(soup)
            if not title: 
                return None
            
            date_str = self._extract_date(soup)
            if not date_str: 
                return None
            
            published_date = self._parse_date(date_str)
            if not published_date: 
                return None
            
            # æ—¥æœŸéæ¿¾
            if self.target_date and published_date < self.target_date:
                return None
            
            # ========== âœ… ä½¿ç”¨æ™ºæ…§æ‘˜è¦ ==========
            paragraphs = self._extract_paragraphs(soup)
            if not paragraphs:
                return None
            
            article_body = TextProcessor.smart_extract_summary(paragraphs)
            
            if len(article_body) < 50:
                return None
            
            author = self._extract_author(soup)
            
            # ========== âœ… æå–é—œéµå­— ==========
            keywords = self._extract_keywords(soup, title, article_body)
            
            # ========== âœ… çµ„è£æ¨™æº–æ ¼å¼ ==========
            return {
                "@type": "NewsArticle",
                "headline": TextProcessor.clean_text(title),
                "articleBody": article_body,  # âœ… æ™ºæ…§æ‘˜è¦
                "author": author or "",  # âœ… å­—ä¸²æ ¼å¼
                "datePublished": published_date.strftime('%Y-%m-%dT%H:%M:%S'),
                "publisher": "ç’°å¢ƒè³‡è¨Šä¸­å¿ƒ",  # âœ… å­—ä¸²æ ¼å¼
                "inLanguage": "zh-TW",
                "url": url,
                "keywords": keywords  # âœ… æ–°å¢æ¬„ä½
            }
            
        except Exception as e:
            print(f"âŒ è§£æéŒ¯èª¤: {e}")
            return None
    
    def _extract_keywords(
        self, 
        soup: BeautifulSoup, 
        title: str, 
        article_body: str
    ) -> List[str]:
        """æå–é—œéµå­—"""
        keywords = []
        
        # æ–¹æ³• 1ï¼šå¾ meta æ¨™ç±¤æå–
        meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
        if meta_keywords and meta_keywords.get('content'):
            content = meta_keywords['content']
            keywords = [
                kw.strip() 
                for kw in re.split(r'[,ï¼Œã€;ï¼›]', content) 
                if kw.strip()
            ]
        
        # æ–¹æ³• 2ï¼šå¾åˆ†é¡æ¨™ç±¤æå–
        if not keywords:
            category_links = soup.select('.field-name-field-category a, .tags a')
            keywords = [
                link.get_text(strip=True) 
                for link in category_links
            ]
        
        # æ–¹æ³• 3ï¼šç°¡æ˜“æå–
        if not keywords:
            keywords = self._simple_keyword_extraction(title)
        
        return keywords[:10]
    
    def _simple_keyword_extraction(self, title: str) -> List[str]:
        """ç°¡æ˜“é—œéµå­—æå–"""
        stopwords = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº',
            'éƒ½', 'ä¸€', 'ä¸€å€‹', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'èªª', 'è¦', 'å»',
            'ä½ ', 'æœƒ', 'è‘—', 'æ²’æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'é€™'
        }
        
        title_clean = re.sub(r'[^\w\s]', ' ', title)
        words = title_clean.split()
        
        keywords = [
            word for word in words 
            if 2 <= len(word) <= 4 and word not in stopwords
        ]
        
        return keywords[:5]
    
    def _extract_title(self, soup: BeautifulSoup) -> Optional[str]:
        title_tag = soup.select_one('h1.title, #page-title')
        if title_tag:
            return title_tag.get_text(strip=True)
        return None
    
    def _extract_date(self, soup: BeautifulSoup) -> Optional[str]:
        date_tag = soup.select_one('.article-create-date')
        if date_tag:
            return date_tag.get_text(strip=True)
        return None
    
    def _parse_date(self, date_str: str) -> Optional[datetime]:
        try:
            match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', date_str)
            if match:
                date_clean = f"{match.group(1)}-{match.group(2).zfill(2)}-{match.group(3).zfill(2)}"
                return datetime.strptime(date_clean, '%Y-%m-%d')
        except Exception:
            pass
        return None
    
    def _extract_paragraphs(self, soup: BeautifulSoup) -> List[str]:
        """æå–å…§æ–‡æ®µè½ï¼ˆç”¨æ–¼æ™ºæ…§æ‘˜è¦ï¼‰"""
        article_tag = soup.select_one('article')
        if not article_tag:
            return []
        
        # ç§»é™¤é›œè¨Šå…ƒç´ 
        for unwanted in article_tag.select(
            '.article-create-date, .share-buttons, '
            '.field-name-field-image, .social-share'
        ):
            unwanted.decompose()
        
        # æå–æ®µè½
        paragraphs = []
        for p in article_tag.find_all(['p', 'div']):
            text = p.get_text(strip=True)
            
            if (text and 
                len(text) > 20 and 
                'è¨‚é–±' not in text and 
                'å»£å‘Š' not in text):
                
                cleaned = TextProcessor.clean_text(text)
                if cleaned:
                    paragraphs.append(cleaned)
        
        return paragraphs
    
    def _extract_author(self, soup: BeautifulSoup) -> Optional[str]:
        article_tag = soup.select_one('article')
        if not article_tag:
            return None
        
        text = article_tag.get_text(strip=True)
        patterns = [
            r'ç’°å¢ƒè³‡è¨Šä¸­å¿ƒè¨˜è€…\s+([^å ±å°]+)å ±å°',
            r'æ–‡ï¼š([^ï¼ˆï¼‰]+)',
            r'ä½œè€…[ï¼š:]\s*([^\n]+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                author_name = match.group(1).strip()
                return TextProcessor.clean_author(author_name)
        
        return None
