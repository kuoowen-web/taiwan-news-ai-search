import json
import asyncio
import aiofiles
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging
import time

from config import settings

class TSVWriter:
    """
    TSV æ ¼å¼è³‡æ–™å¯«å…¥å™¨
    åƒè€ƒèˆŠç‰ˆçš„ save_to_tsv_async é‚è¼¯
    å°‡çˆ¬å–çš„è³‡æ–™ä»¥ TSV æ ¼å¼å¯«å…¥æª”æ¡ˆ
    """
    
    def __init__(self, source_name: str, output_dir: Optional[Path] = None, filename: Optional[str] = None):
        """
        åˆå§‹åŒ– TSV å¯«å…¥å™¨
        
        Args:
            source_name: æ–°èä¾†æºåç¨±
            output_dir: è¼¸å‡ºç›®éŒ„ï¼ˆå¯é¸ï¼Œé è¨­ä½¿ç”¨è¨­å®šä¸­çš„ç›®éŒ„ï¼‰
            filename: è¼¸å‡ºæª”æ¡ˆåç¨±ï¼ˆå¯é¸ï¼Œé è¨­ä½¿ç”¨æ™‚é–“æˆ³ï¼‰
        """
        self.source_name = source_name
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # è¨­å®šè¼¸å‡ºç›®éŒ„
        self.output_dir = output_dir if output_dir else settings.OUTPUT_DIR
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è¨­å®šæª”æ¡ˆåç¨±ï¼ˆåƒè€ƒèˆŠç‰ˆæ ¼å¼ï¼‰
        if filename:
            self.filename = filename
        else:
            timestamp = time.strftime('%Y-%m-%d_%H-%M')
            self.filename = f"{source_name}_{timestamp}.tsv"
        
        self.output_path = self.output_dir / self.filename
        
        # ğŸ†• è¨­å®š ID è¨˜éŒ„æª”è·¯å¾‘
        settings.CRAWLED_IDS_DIR.mkdir(parents=True, exist_ok=True)
        self.ids_filename = f"{source_name}.txt"
        self.ids_path = settings.CRAWLED_IDS_DIR / self.ids_filename
        
        # è¨­å®šé–ï¼Œé˜²æ­¢å¤šç·šç¨‹å¯«å…¥è¡çªï¼ˆåƒè€ƒèˆŠç‰ˆï¼‰
        self.lock = asyncio.Lock()
        # ğŸ†• ID æª”æ¡ˆå°ˆç”¨é–
        self.ids_lock = asyncio.Lock()
        
        self.logger.info(f"TSVWriter initialized: {self.output_path}")
        self.logger.info(f"ID tracker initialized: {self.ids_path}")
    
    async def save_item(self, url: str, data: Dict[str, Any]) -> bool:
        """
        å„²å­˜å–®ç­†è³‡æ–™
        æ ¼å¼ï¼šURL \t JSON_STRING
        
        Args:
            url: æ–‡ç« URL
            data: æ–‡ç« è³‡æ–™å­—å…¸
            
        Returns:
            æˆåŠŸè¿”å›Trueï¼Œå¤±æ•—è¿”å›False
        """
        try:
            # ç¢ºä¿JSONå­—ä¸²ä½¿ç”¨ASCIIç·¨ç¢¼ï¼Œä¸­æ–‡è½‰ç‚ºUnicode escape
            # åƒè€ƒèˆŠç‰ˆï¼šensure_ascii=True
            json_str = json.dumps(data, ensure_ascii=settings.ENSURE_ASCII, separators=(',', ':'))
            
            # ä½¿ç”¨é–ç¢ºä¿å¯«å…¥æ“ä½œçš„åŸå­æ€§
            async with self.lock:
                # ä½¿ç”¨ aiofiles é€²è¡ŒéåŒæ­¥å¯«å…¥
                async with aiofiles.open(self.output_path, 'a', encoding='utf-8') as f:
                    # æ ¼å¼ï¼šURL \t JSON_STRING \n
                    await f.write(f"{url}\t{json_str}\n")
            
            # ğŸ†• æˆåŠŸå¯«å…¥è³‡æ–™å¾Œï¼Œè¨˜éŒ„ ID
            await self._save_crawled_id(url)
            
            return True
            
        except Exception as e:
            self.logger.error(f"Error saving data for {url}: {str(e)}")
            if settings.DEBUG:
                import traceback
                self.logger.error(traceback.format_exc())
            return False
    
    async def _save_crawled_id(self, url: str) -> bool:
        """
        ğŸ†• å°‡å·²çˆ¬å–çš„ URL è¨˜éŒ„åˆ° ID æª”æ¡ˆ
        
        Args:
            url: æ–‡ç«  URL æˆ– ID
            
        Returns:
            æˆåŠŸè¿”å› Trueï¼Œå¤±æ•—è¿”å› False
        """
        try:
            async with self.ids_lock:
                async with aiofiles.open(self.ids_path, 'a', encoding='utf-8') as f:
                    await f.write(f"{url}\n")
            return True
        except Exception as e:
            self.logger.error(f"Error saving crawled ID {url}: {str(e)}")
            return False
    
    async def save_batch(self, data_list: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        å„²å­˜å¤šç­†è³‡æ–™
        
        Args:
            data_list: åŒ…å« {'url': ..., 'data': ...} çš„åˆ—è¡¨
            
        Returns:
            åŒ…å«æˆåŠŸå’Œå¤±æ•—è¨ˆæ•¸çš„å­—å…¸
        """
        results = {
            'total': len(data_list),
            'success': 0,
            'failed': 0,
            'failed_urls': []
        }
        
        for item in data_list:
            # ç¢ºä¿æ¯å€‹é …ç›®éƒ½æœ‰ url å’Œ data æ¬„ä½
            if 'url' not in item or 'data' not in item:
                self.logger.error(f"Invalid data format: {item}")
                results['failed'] += 1
                if 'url' in item:
                    results['failed_urls'].append(item['url'])
                continue
            
            success = await self.save_item(item['url'], item['data'])
            if success:
                results['success'] += 1
            else:
                results['failed'] += 1
                results['failed_urls'].append(item['url'])
        
        self.logger.info(
            f"Batch save completed: {results['success']}/{results['total']} successful"
        )
        if results['failed'] > 0:
            self.logger.warning(f"Failed to save {results['failed']} items")
        
        return results

class Pipeline:
    """
    è³‡æ–™è™•ç†ç®¡é“
    å”èª¿è³‡æ–™çš„è™•ç†å’Œå­˜å„²
    """
    
    def __init__(self, source_name: str, output_dir: Optional[Path] = None, filename: Optional[str] = None):
        """
        åˆå§‹åŒ–ç®¡é“
        
        Args:
            source_name: æ–°èä¾†æºåç¨±
            output_dir: è¼¸å‡ºç›®éŒ„ï¼ˆå¯é¸ï¼‰
            filename: è¼¸å‡ºæª”æ¡ˆåç¨±ï¼ˆå¯é¸ï¼‰
        """
        self.writer = TSVWriter(source_name, output_dir, filename)
        self.logger = logging.getLogger(self.__class__.__name__)
    
    async def process_and_save(self, url: str, data: Dict[str, Any]) -> bool:
        """
        è™•ç†ä¸¦å„²å­˜å–®ç­†è³‡æ–™
        
        Args:
            url: æ–‡ç« URL
            data: æ–‡ç« è³‡æ–™
            
        Returns:
            æˆåŠŸè¿”å›Trueï¼Œå¤±æ•—è¿”å›False
        """
        # é€™è£¡å¯ä»¥æ·»åŠ é¡å¤–çš„è³‡æ–™è™•ç†é‚è¼¯
        # ä¾‹å¦‚ï¼šè³‡æ–™æ¸…æ´—ã€æ ¼å¼è½‰æ›ç­‰
        # ç›®å‰ç›´æ¥å„²å­˜
        
        return await self.writer.save_item(url, data)
    
    async def process_and_save_batch(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        è™•ç†ä¸¦å„²å­˜å¤šç­†è³‡æ–™
        
        Args:
            results: åŒ…å« {'url': ..., 'data': ...} çš„åˆ—è¡¨
            
        Returns:
            åŒ…å«æˆåŠŸå’Œå¤±æ•—è¨ˆæ•¸çš„å­—å…¸
        """
        return await self.writer.save_batch(results)
