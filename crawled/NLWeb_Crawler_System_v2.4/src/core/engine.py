import asyncio
import aiohttp
import logging
import random
import time
from typing import Dict, List, Optional, Any, Set, Union
from pathlib import Path
from datetime import datetime, timedelta
from enum import Enum

from config import settings
from config.settings import DEFAULT_HEADERS
from src.core.interfaces import BaseParser
from src.core.pipeline import Pipeline

# âœ… FIX #ENGINE-HYBRID-SESSION: å¼•å…¥ curl_cffi æ”¯æ´
try:
    from curl_cffi.requests import AsyncSession as CurlSession
    CURL_CFFI_AVAILABLE = True
except ImportError:
    CURL_CFFI_AVAILABLE = False
    CurlSession = None

# âœ… FIX #ENGINE-REFACTOR-V2: å¼•å…¥ç‹€æ…‹åˆ—èˆ‰
class CrawlStatus(Enum):
    """
    çˆ¬å–ç‹€æ…‹åˆ—èˆ‰
    
    ç”¨é€”ï¼šç²¾ç¢ºåˆ†é¡æ¯æ¬¡çˆ¬å–çš„çµæœï¼Œé¿å…èª¤åˆ¤
    
    âœ… FIX #ENGINE-REFACTOR-V2: æ ¸å¿ƒæ”¹é€²
    - SUCCESS: 200 OKï¼ŒæˆåŠŸçˆ¬å–
    - NOT_FOUND: 404 Not Foundï¼Œæ–‡ç« ä¸å­˜åœ¨ï¼ˆçœŸçš„æ²’æœ‰ï¼‰
    - BLOCKED: 403/429/5xx/Timeoutï¼Œè¢«å°é–æˆ–ç¶²è·¯å•é¡Œï¼ˆå¯èƒ½æœ‰è³‡æ–™ï¼‰
    """
    SUCCESS = "SUCCESS"
    NOT_FOUND = "NOT_FOUND"
    BLOCKED = "BLOCKED"

# âœ… FIX #ENGINE-HYBRID-SESSION: Session é¡å‹åˆ—èˆ‰
class SessionType(Enum):
    """Session é¡å‹åˆ—èˆ‰"""
    AIOHTTP = "aiohttp"
    CURL_CFFI = "curl_cffi"

class CrawlerEngine:
    """
    é€šç”¨çˆ¬èŸ²å¼•æ“
    
    âœ… FIX #WORK-ORDER-921: æ”¯æ´ä¾†æºå°ˆå±¬è¨­å®š (Source-Specific Config)
    âœ… FIX #WORK-ORDER-912: ä¿®å¾©æ—¥æœŸè§£æèˆ‡è·³èºé‚è¼¯é™¤éŒ¯
    âœ… FIX #WORK-ORDER-911: ä¿®æ­£æ™ºèƒ½è·³èºè¨ˆæ•¸é‚è¼¯èˆ‡è¦–è¦ºåŒ–
    âœ… FIX #WORK-ORDER-906: åŠ å…¥é™¤éŒ¯ Log è¿½è¹¤ Duck Typing
    âœ… FIX #WORK-ORDER-902: æ”¯æ´åˆ—è¡¨å¼çˆ¬å– (List-Based Crawling)
    âœ… FIX #WORK-ORDER-902: ä¿®å¾© Pipeline.close() AttributeError
    âœ… FIX #WORK-ORDER-806: è™•ç† Parser å›å‚³ None çš„æƒ…æ³
    âœ… FIX #ENGINE-SAFETY-PATCH: æé«˜æ™ºèƒ½è·³èºå®¹å¿åº¦
    âœ… FIX #ENGINE-FIX-001: æ•´åˆå…¨åŸŸå½è£æ¨™é ­
    âœ… FIX #ENGINE-REFACTOR-V2: é‡æ§‹ç‹€æ…‹åˆ†é¡é‚è¼¯
    âœ… FIX #ENGINE-HYBRID-SESSION: æ”¯æ´ aiohttp å’Œ curl_cffi
    âœ… FIX #ENGINE-CLOSE-TIMEOUT: åŠ ä¸Š close() timeout ä¿è­·
    
    è¨­è¨ˆåŸå‰‡ï¼š
    1. ä¾è³´æ³¨å…¥ï¼šé€é BaseParser ä»‹é¢èˆ‡å…·é«”ç¶²ç«™è§£è€¦
    2. é—œæ³¨é»åˆ†é›¢ï¼šåªè² è²¬çˆ¬å–æµç¨‹ï¼Œè§£æé‚è¼¯å§”è¨—çµ¦ Parser
    3. å¯é‡ç”¨ï¼šé©ç”¨æ–¼æ‰€æœ‰å¯¦ä½œ BaseParser çš„ç¶²ç«™
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    - ç¯„åœçˆ¬å–ï¼šrun_range(start_id, end_id)
    - åˆ—è¡¨çˆ¬å–ï¼šrun_list(url_list)
    - è‡ªå‹•çˆ¬å–ï¼šrun_auto(count) - æ”¯æ´åˆ—è¡¨å¼å’Œæµæ°´è™Ÿå¼
    - ä½µç™¼æ§åˆ¶ï¼šä½¿ç”¨ Semaphore é™åˆ¶åŒæ™‚è«‹æ±‚æ•¸ï¼ˆæ”¯æ´ä¾†æºå°ˆå±¬è¨­å®šï¼‰
    - é‡è©¦æ©Ÿåˆ¶ï¼šè™•ç†ç¶²è·¯éŒ¯èª¤å’Œè‡¨æ™‚å¤±æ•—
    - å»é‡æ©Ÿåˆ¶ï¼šé¿å…é‡è¤‡çˆ¬å–
    - è‡ªå‹•å„²å­˜ï¼šæ•´åˆ Pipeline
    - æ™ºèƒ½è·³èºï¼šåµæ¸¬ç©ºçª—æœŸä¸¦è·³åˆ°ä¸‹ä¸€å¤©ï¼ˆChinaTimes/CNAï¼Œé–¾å€¼ 100ï¼‰
    - ç‹€æ…‹åˆ†é¡ï¼šå€åˆ† 404ï¼ˆä¸å­˜åœ¨ï¼‰èˆ‡ 403/429ï¼ˆè¢«å°é–ï¼‰
    - æ··åˆ Sessionï¼šæ ¹æ“šä¾†æºè‡ªå‹•é¸æ“‡ aiohttp æˆ– curl_cffi
    - None å®¹éŒ¯ï¼šè™•ç† Parser å›å‚³ None çš„æƒ…æ³ï¼ˆMOEA åˆ—è¡¨ç­–ç•¥ï¼‰
    - ä¾†æºå°ˆå±¬è¨­å®šï¼šå„ªå…ˆè®€å– NEWS_SOURCES ä¸­çš„ concurrent_limit å’Œ delay_range
    """
    
    SMART_JUMP_THRESHOLD = 100  # âœ… é€£çºŒå¤±æ•— 100 æ¬¡è§¸ç™¼è·³èº
    
    def __init__(
        self,
        parser: BaseParser,
        session: Optional[Union[aiohttp.ClientSession, 'CurlSession']] = None,
        auto_save: bool = True
    ):
        """
        åˆå§‹åŒ–çˆ¬èŸ²å¼•æ“
        
        âœ… FIX #WORK-ORDER-921: å„ªå…ˆè®€å–ä¾†æºå°ˆå±¬è¨­å®š
        âœ… FIX #ENGINE-HYBRID-SESSION: æ”¯æ´å¤šç¨® Session é¡å‹
        
        Args:
            parser: BaseParser å¯¦ä¾‹ï¼ˆå¿…é ˆï¼‰
            session: aiohttp.ClientSession æˆ– curl_cffi.AsyncSession å¯¦ä¾‹ï¼ˆå¯é¸ï¼‰
            auto_save: æ˜¯å¦è‡ªå‹•å„²å­˜çˆ¬å–çµæœï¼ˆé è¨­ Trueï¼‰
        """
        self.parser = parser
        self.session = session
        self.auto_save = auto_save
        
        # âœ… FIX #WORK-ORDER-921: è¼‰å…¥ä¾†æºå°ˆå±¬è¨­å®š
        self._load_source_config()
        
        # âœ… FIX #ENGINE-HYBRID-SESSION: åˆ¤æ–· Session é¡å‹
        if session is not None:
            if CURL_CFFI_AVAILABLE and isinstance(session, CurlSession):
                self.session_type = SessionType.CURL_CFFI
            else:
                self.session_type = SessionType.AIOHTTP
        else:
            # æ ¹æ“šä¾†æºè‡ªå‹•é¸æ“‡ï¼ˆå¯åœ¨ settings ä¸­é…ç½®ï¼‰
            if hasattr(settings, 'CURL_CFFI_SOURCES') and parser.source_name in settings.CURL_CFFI_SOURCES:
                self.session_type = SessionType.CURL_CFFI
            else:
                self.session_type = SessionType.AIOHTTP
        
        # è¨­å®šæ—¥èªŒ
        self.logger = logging.getLogger(f"{self.__class__.__name__}_{parser.source_name}")
        if not self.logger.handlers:
            self._setup_logger()
        
        self.logger.info(f"Engine initialized with session type: {self.session_type.value}")
        
        # âœ… FIX #WORK-ORDER-921: é¡¯ç¤ºä¾†æºå°ˆå±¬è¨­å®š
        self.logger.info(f"   Concurrent limit: {self.concurrent_limit}")
        self.logger.info(f"   Delay range: {self.min_delay:.1f}s - {self.max_delay:.1f}s")
        
        # åˆå§‹åŒ– Pipeline
        if self.auto_save:
            self.pipeline = Pipeline(source_name=parser.source_name)
        
        # è¼‰å…¥æ­·å²è¨˜éŒ„ï¼ˆå»é‡ï¼‰
        self.crawled_ids: Set[str] = set()
        self._load_history()
        
        # âœ… FIX #ENGINE-REFACTOR-V2: æ›´æ–°çµ±è¨ˆè³‡è¨Š
        self.stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'skipped': 0,
            'not_found': 0,
            'blocked': 0,
        }
        
        # æ™ºèƒ½è·³èºç‹€æ…‹
        self.consecutive_failures = 0  # âœ… è¨ˆç®—æ‰€æœ‰å¤±æ•—ï¼ˆNOT_FOUND + BLOCKEDï¼‰
        self.smart_jump_count = 0
        
        # 429 é™é€Ÿç‹€æ…‹
        self.rate_limit_hit = False
        self.rate_limit_cooldown_until = 0
    
    def _load_source_config(self) -> None:
        """
        è¼‰å…¥ä¾†æºå°ˆå±¬è¨­å®š
        
        âœ… FIX #WORK-ORDER-921: æ–°å¢æ–¹æ³•
        
        å„ªå…ˆé †åºï¼š
        1. settings.NEWS_SOURCES[source_name]['concurrent_limit']
        2. settings.CONCURRENT_REQUESTSï¼ˆå…¨åŸŸé è¨­å€¼ï¼‰
        
        åŒç†é©ç”¨æ–¼ delay_range
        """
        source_name = self.parser.source_name
        
        # å˜—è©¦å¾ NEWS_SOURCES è®€å–ä¾†æºå°ˆå±¬è¨­å®š
        if hasattr(settings, 'NEWS_SOURCES') and source_name in settings.NEWS_SOURCES:
            source_config = settings.NEWS_SOURCES[source_name]
            
            # è®€å– concurrent_limit
            self.concurrent_limit = source_config.get('concurrent_limit', settings.CONCURRENT_REQUESTS)
            
            # è®€å– delay_range
            delay_range = source_config.get('delay_range', (settings.MIN_DELAY, settings.MAX_DELAY))
            self.min_delay, self.max_delay = delay_range
            
        else:
            # é™ç´šç‚ºå…¨åŸŸé è¨­å€¼
            self.concurrent_limit = settings.CONCURRENT_REQUESTS
            self.min_delay = settings.MIN_DELAY
            self.max_delay = settings.MAX_DELAY
    
    def _setup_logger(self) -> None:
        """è¨­ç½®æ—¥èªŒè™•ç†å™¨"""
        settings.LOG_DIR.mkdir(parents=True, exist_ok=True)
        
        log_file = settings.LOG_DIR / f"engine_{self.parser.source_name}_{time.strftime('%Y%m%d')}.log"
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        console_handler = logging.StreamHandler()
        
        formatter = logging.Formatter(
            settings.LOG_FORMAT,
            datefmt=settings.LOG_DATE_FORMAT
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
        self.logger.setLevel(settings.LOG_LEVEL)
    
    def _load_history(self) -> int:
        """
        è¼‰å…¥æ­·å²å·²çˆ¬å–çš„ URL è¨˜éŒ„
        
        Returns:
            è¼‰å…¥çš„ URL æ•¸é‡
        """
        try:
            if not settings.CRAWLED_IDS_DIR.exists():
                self.logger.info(f"No history file found for {self.parser.source_name}, starting fresh")
                return 0
            
            ids_file = settings.CRAWLED_IDS_DIR / f"{self.parser.source_name}.txt"
            
            if not ids_file.exists():
                self.logger.info(f"No history file found for {self.parser.source_name}, starting fresh")
                return 0
            
            with open(ids_file, 'r', encoding='utf-8') as f:
                for line in f:
                    url = line.strip()
                    if url:
                        self.crawled_ids.add(url)
            
            count = len(self.crawled_ids)
            self.logger.info(f"ğŸ“‚ Loaded {count:,} crawled URLs from history")
            return count
            
        except Exception as e:
            self.logger.error(f"Error loading history: {str(e)}")
            if settings.DEBUG:
                import traceback
                self.logger.error(traceback.format_exc())
            return 0
    
    def _is_crawled(self, url: str) -> bool:
        """æª¢æŸ¥ URL æ˜¯å¦å·²çˆ¬å–"""
        return url in self.crawled_ids
    
    def _mark_as_crawled(self, url: str) -> None:
        """æ¨™è¨˜ URL ç‚ºå·²çˆ¬å–"""
        self.crawled_ids.add(url)
    
    async def _create_session(self) -> Union[aiohttp.ClientSession, 'CurlSession']:
        """
        å‰µå»º Sessionï¼ˆå·¥å» æ–¹æ³•ï¼‰
        
        âœ… FIX #ENGINE-HYBRID-SESSION: æ ¹æ“š session_type é¸æ“‡
        âœ… FIX #ENGINE-FIX-001: è‡ªå‹•å¥—ç”¨å…¨åŸŸé è¨­ Headers
        
        Returns:
            aiohttp.ClientSession æˆ– curl_cffi.AsyncSession
        """
        if self.session_type == SessionType.CURL_CFFI:
            if not CURL_CFFI_AVAILABLE:
                self.logger.warning("curl_cffi not available, falling back to aiohttp")
                self.session_type = SessionType.AIOHTTP
            else:
                self.logger.info("Creating curl_cffi session")
                return CurlSession(
                    headers=DEFAULT_HEADERS,
                    timeout=settings.REQUEST_TIMEOUT,
                    impersonate="chrome110"  # å½è£ç‚º Chrome 110
                )
        
        # é è¨­ä½¿ç”¨ aiohttp
        self.logger.info("Creating aiohttp session")
        import ssl
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        connector = aiohttp.TCPConnector(ssl=ssl_context)
        
        return aiohttp.ClientSession(
            connector=connector,
            timeout=aiohttp.ClientTimeout(total=settings.REQUEST_TIMEOUT),
            headers=DEFAULT_HEADERS
        )
    
    def _get_headers(self) -> Dict[str, str]:
        """
        ç²å–è«‹æ±‚æ¨™é ­ï¼ˆæ”¯æ´å‹•æ…‹ User-Agent è¼ªæ›ï¼‰
        
        ç­–ç•¥ï¼š
        1. åŸºæ–¼å…¨åŸŸ DEFAULT_HEADERS
        2. éš¨æ©Ÿæ›¿æ› User-Agentï¼ˆå¾ USER_AGENTS æ± é¸æ“‡ï¼‰
        3. ä¿ç•™å…¶ä»–æ¨™é ­ä¸è®Š
        
        Returns:
            å®Œæ•´çš„ HTTP Headers å­—å…¸
        """
        headers = DEFAULT_HEADERS.copy()
        headers['User-Agent'] = random.choice(settings.USER_AGENTS)
        return headers
    
    def _parse_date_from_id(self, article_id: int) -> Optional[datetime]:
        """
        å¾æ—¥æœŸå‹ ID ä¸­è§£ææ—¥æœŸï¼ˆç”¨æ–¼æ™ºèƒ½è·³èºï¼‰
        
        âœ… FIX #WORK-ORDER-912: åŠ å…¥è©³ç´°é™¤éŒ¯ Logï¼Œæ”¯æ´ 8/12/14 ç¢¼
        
        æ”¯æ´æ ¼å¼ï¼š
        - 8 ç¢¼ï¼šYYYYMMDD (å¦‚ 20251231)
        - 12 ç¢¼ï¼šYYYYMMDDxxxx (å¦‚ 202512310390ï¼ŒCNA æ ¼å¼)
        - 14 ç¢¼ï¼šYYYYMMDDHHmmss (å¦‚ 20251231235959ï¼ŒChinaTimes æ ¼å¼)
        
        Args:
            article_id: æ–‡ç«  ID
            
        Returns:
            è§£æå‡ºçš„æ—¥æœŸï¼Œæˆ– Noneï¼ˆå¦‚æœç„¡æ³•è§£æï¼‰
        """
        try:
            id_str = str(article_id)
            
            # âœ… ç¢ºä¿è‡³å°‘æœ‰ 8 ç¢¼ (YYYYMMDD)
            if len(id_str) >= 8:
                date_str = id_str[:8]  # åªå–å‰ 8 ç¢¼
                
                # âœ… FIX #WORK-ORDER-912: åŠ å…¥é™¤éŒ¯ Log
                self.logger.debug(f"[Date Parse] ID: {article_id} (len={len(id_str)}), extracted: {date_str}")
                
                parsed_date = datetime.strptime(date_str, '%Y%m%d')
                
                self.logger.debug(f"[Date Parse] Success: {parsed_date.strftime('%Y-%m-%d')}")
                return parsed_date
            else:
                self.logger.warning(f"[Date Parse] ID too short: {article_id} (len={len(id_str)} < 8)")
                return None
                
        except (ValueError, IndexError) as e:
            # âœ… FIX #WORK-ORDER-912: è©³ç´°éŒ¯èª¤è¨Šæ¯
            self.logger.error(f"[Date Parse] Failed for ID {article_id}: {e}")
            return None
        
        return None
    
    def _calculate_jump_target(self, current_id: int) -> Optional[int]:
        """
        è¨ˆç®—æ™ºèƒ½è·³èºçš„ç›®æ¨™ IDï¼ˆè·³åˆ°ä¸‹ä¸€å¤©ï¼‰
        
        âœ… FIX #WORK-ORDER-912: åŠ å…¥è©³ç´°é™¤éŒ¯ Log
        
        Args:
            current_id: ç•¶å‰ ID
            
        Returns:
            è·³èºç›®æ¨™ IDï¼Œæˆ– Noneï¼ˆå¦‚æœç„¡æ³•è¨ˆç®—ï¼‰
        """
        self.logger.debug(f"[Jump Calc] Calculating jump target from ID: {current_id}")
        
        current_date = self._parse_date_from_id(current_id)
        
        if current_date is None:
            self.logger.warning(f"[Jump Calc] Failed: Cannot parse date from ID {current_id}")
            return None
        
        # è·³åˆ°ä¸‹ä¸€å¤©çš„ 00:00:00
        next_day = current_date + timedelta(days=1)
        
        # âœ… æ ¹æ“š ID é•·åº¦æ±ºå®šè·³èºæ ¼å¼
        id_str = str(current_id)
        id_len = len(id_str)
        
        if id_len == 8:
            # 8 ç¢¼æ ¼å¼ï¼šYYYYMMDD
            jump_target_id = int(next_day.strftime('%Y%m%d'))
        elif id_len == 12:
            # 12 ç¢¼æ ¼å¼ï¼šYYYYMMDDxxxx (CNA)
            jump_target_id = int(next_day.strftime('%Y%m%d') + '0001')
        elif id_len == 14:
            # 14 ç¢¼æ ¼å¼ï¼šYYYYMMDDHHmmss (ChinaTimes)
            jump_target_id = int(next_day.strftime('%Y%m%d') + '000000')
        else:
            # é è¨­ï¼šä½¿ç”¨ 14 ç¢¼æ ¼å¼
            jump_target_id = int(next_day.strftime('%Y%m%d') + '000000')
        
        self.logger.debug(f"[Jump Calc] Success: {current_id} -> {jump_target_id} (next day: {next_day.strftime('%Y-%m-%d')})")
        
        return jump_target_id
    
    def _should_enable_smart_jump(self) -> bool:
        """
        åˆ¤æ–·æ˜¯å¦æ‡‰è©²å•Ÿç”¨æ™ºèƒ½è·³èº
        
        Returns:
            True å¦‚æœæ‡‰è©²å•Ÿç”¨æ™ºèƒ½è·³èºï¼ˆChinaTimes/CNAï¼‰
        """
        return self.parser.source_name in settings.SMART_JUMP_ENABLED_SOURCES
    
    async def _handle_rate_limit(self) -> None:
        """
        è™•ç† 429 Rate Limit éŒ¯èª¤
        
        å‹•æ…‹é™é€Ÿæ©Ÿåˆ¶
        """
        self.rate_limit_hit = True
        cooldown = settings.RATE_LIMIT_COOLDOWN
        
        self.logger.warning(f"âš ï¸  Rate limit detected (429), cooling down for {cooldown}s...")
        self.rate_limit_cooldown_until = time.time() + cooldown
        
        await asyncio.sleep(cooldown)
        
        self.rate_limit_hit = False
        self.logger.info(f"âœ… Cooldown completed, resuming...")
    
    async def _fetch(
        self, 
        url: str, 
        session: Union[aiohttp.ClientSession, 'CurlSession']
    ) -> tuple[Optional[str], CrawlStatus]:
        """
        ç²å– URL å…§å®¹ï¼ŒåŒ…å«é‡è©¦æ©Ÿåˆ¶
        
        âœ… FIX #ENGINE-REFACTOR-V2: å›å‚³ CrawlStatus
        âœ… FIX #ENGINE-REFACTOR-V2: Timeout ç›´æ¥è¦–ç‚º NOT_FOUNDï¼ˆåŠ é€Ÿç„¡æ•ˆ IDï¼‰
        âœ… FIX #ENGINE-HYBRID-SESSION: å…¼å®¹ aiohttp å’Œ curl_cffi
        
        Args:
            url: è¦ç²å–çš„ URL
            session: aiohttp.ClientSession æˆ– curl_cffi.AsyncSession
            
        Returns:
            (HTML å…§å®¹, CrawlStatus)
        """
        # æª¢æŸ¥æ˜¯å¦åœ¨å†·å»æœŸ
        if self.rate_limit_hit:
            wait_time = self.rate_limit_cooldown_until - time.time()
            if wait_time > 0:
                self.logger.debug(f"Waiting for rate limit cooldown: {wait_time:.2f}s")
                await asyncio.sleep(wait_time)
        
        retry_count = 0
        max_retries = settings.MAX_RETRIES
        last_error_type = None
        
        while retry_count <= max_retries:
            try:
                headers = self._get_headers()
                
                # âœ… FIX #ENGINE-HYBRID-SESSION: å…¼å®¹å…©ç¨® Session API
                if self.session_type == SessionType.CURL_CFFI:
                    # curl_cffi API
                    response = await session.get(url, headers=headers)
                    status = response.status_code
                    
                    if status == 200:
                        html = response.text
                        return (html, CrawlStatus.SUCCESS)
                    
                    elif status == 404:
                        self.logger.debug(f"Page not found (404): {url}")
                        return (None, CrawlStatus.NOT_FOUND)
                    
                    elif status in (403, 429):
                        self.logger.warning(f"âš ï¸  Blocked ({status}) for {url}")
                        last_error_type = 'blocked'
                        await self._handle_rate_limit()
                        # ç¹¼çºŒé‡è©¦
                    
                    elif status in (500, 502, 503, 504):
                        self.logger.warning(f"Server error ({status}) for {url}")
                        last_error_type = 'server_error'
                        # ç¹¼çºŒåˆ°é‡è©¦é‚è¼¯
                    
                    else:
                        self.logger.warning(f"Unexpected status {status} for {url}")
                        return (None, CrawlStatus.BLOCKED)
                
                else:
                    # aiohttp API
                    async with session.get(
                        url,
                        headers=headers,
                        timeout=aiohttp.ClientTimeout(total=settings.REQUEST_TIMEOUT)
                    ) as response:
                        if response.status == 200:
                            html = await response.text()
                            return (html, CrawlStatus.SUCCESS)
                        
                        elif response.status == 404:
                            self.logger.debug(f"Page not found (404): {url}")
                            return (None, CrawlStatus.NOT_FOUND)
                        
                        elif response.status in (403, 429):
                            self.logger.warning(f"âš ï¸  Blocked ({response.status}) for {url}")
                            last_error_type = 'blocked'
                            await self._handle_rate_limit()
                            # ç¹¼çºŒé‡è©¦
                        
                        elif response.status in (500, 502, 503, 504):
                            self.logger.warning(f"Server error ({response.status}) for {url}")
                            last_error_type = 'server_error'
                            # ç¹¼çºŒåˆ°é‡è©¦é‚è¼¯
                        
                        else:
                            self.logger.warning(f"Unexpected status {response.status} for {url}")
                            return (None, CrawlStatus.BLOCKED)
            
            except asyncio.TimeoutError:
                # âœ… FIX #ENGINE-REFACTOR-V2: Timeout ç›´æ¥è¦–ç‚º NOT_FOUND
                # åŸå› ï¼šé‡å°ä¸­æ™‚ 000001 ç­‰ç„¡æ•ˆè™Ÿï¼Œä¸è¦è®“å®ƒå¡ä½é‡è©¦
                self.logger.debug(f"Timeout for {url}, treating as NOT_FOUND")
                return (None, CrawlStatus.NOT_FOUND)
            
            except (aiohttp.ClientError, Exception) as e:
                # å…¼å®¹ curl_cffi çš„ç•°å¸¸
                self.logger.debug(f"Network error fetching {url}: {str(e)}")
                last_error_type = 'network_error'
                # ç¹¼çºŒåˆ°é‡è©¦é‚è¼¯
            
            # é‡è©¦é‚è¼¯
            retry_count += 1
            if retry_count <= max_retries:
                wait_time = self._calculate_retry_delay(retry_count)
                self.logger.debug(f"Retrying {url} in {wait_time:.2f}s... ({retry_count}/{max_retries})")
                await asyncio.sleep(wait_time)
            else:
                self.logger.warning(f"Max retries reached for {url} (reason: {last_error_type})")
        
        # âœ… é‡è©¦å¤±æ•—ï¼Œæ¨™è¨˜ç‚º BLOCKED
        return (None, CrawlStatus.BLOCKED)
    
    def _calculate_retry_delay(self, retry_count: int) -> float:
        """è¨ˆç®—æŒ‡æ•¸é€€é¿å»¶é²æ™‚é–“"""
        delay = settings.RETRY_DELAY * (2 ** (retry_count - 1))
        jitter = delay * 0.2 * (random.random() * 2 - 1)
        delay += jitter
        return min(delay, settings.MAX_RETRY_DELAY)
    
    async def _random_delay(self):
        """
        éš¨æ©Ÿå»¶é²ï¼Œé¿å…è¢«åµæ¸¬ç‚ºçˆ¬èŸ²
        
        âœ… FIX #WORK-ORDER-921: ä½¿ç”¨ä¾†æºå°ˆå±¬çš„ delay_range
        """
        await asyncio.sleep(random.uniform(self.min_delay, self.max_delay))
    
    async def _process_article(
        self,
        article_id: int,
        session: Union[aiohttp.ClientSession, 'CurlSession']
    ) -> CrawlStatus:
        """
        è™•ç†å–®ç¯‡æ–‡ç« 
        
        âœ… FIX #WORK-ORDER-806: è™•ç† Parser å›å‚³ None çš„æƒ…æ³
        âœ… FIX #ENGINE-REFACTOR-V2: å›å‚³ CrawlStatus
        
        Args:
            article_id: æ–‡ç«  ID
            session: aiohttp.ClientSession æˆ– curl_cffi.AsyncSession
            
        Returns:
            CrawlStatusï¼ˆSUCCESS / NOT_FOUND / BLOCKEDï¼‰
        """
        # âœ… FIX #WORK-ORDER-806: ç”Ÿæˆ URLï¼ˆå¯èƒ½å›å‚³ Noneï¼‰
        url = self.parser.get_url(article_id)
        
        # âœ… FIX #WORK-ORDER-806: å¦‚æœ Parser å›å‚³ Noneï¼Œç›´æ¥è·³é
        if not url:
            self.logger.debug(f"â­ï¸  Skipping ID {article_id:,}: Parser returned no URL (Not in cache)")
            self.stats['not_found'] += 1
            return CrawlStatus.NOT_FOUND
        
        # æª¢æŸ¥æ˜¯å¦å·²çˆ¬å–
        if self._is_crawled(url):
            self.logger.debug(f"â­ï¸  Skipping already crawled: {url}")
            self.stats['skipped'] += 1
            return CrawlStatus.SUCCESS  # è¦–ç‚ºæˆåŠŸï¼ˆä¸å½±éŸ¿è¨ˆæ•¸ï¼‰
        
        # ç²å– HTML
        html, status = await self._fetch(url, session)
        
        # âœ… FIX #ENGINE-REFACTOR-V2: æ ¹æ“šç‹€æ…‹è™•ç†
        if status == CrawlStatus.NOT_FOUND:
            self.stats['not_found'] += 1
            return CrawlStatus.NOT_FOUND
        
        elif status == CrawlStatus.BLOCKED:
            self.stats['blocked'] += 1
            return CrawlStatus.BLOCKED
        
        # status == CrawlStatus.SUCCESS
        if html is None:
            self.stats['failed'] += 1
            return CrawlStatus.BLOCKED
        
        # è§£æ HTML
        try:
            data = await self.parser.parse(html, url)
            if data is None:
                self.logger.debug(f"Parser returned None for {url}")
                self.stats['failed'] += 1
                return CrawlStatus.NOT_FOUND
            
            # æ¨™è¨˜ç‚ºå·²çˆ¬å–
            self._mark_as_crawled(url)
            
            # è‡ªå‹•å„²å­˜
            if self.auto_save:
                success = await self.pipeline.process_and_save(url, data)
                if success:
                    self.logger.info(f"âœ… Parsed ID: {article_id:,}")
                    self.stats['success'] += 1
                else:
                    self.logger.error(f"âŒ Failed to save ID: {article_id:,}")
                    self.stats['failed'] += 1
            else:
                self.logger.info(f"âœ… Parsed ID: {article_id:,}")
                self.stats['success'] += 1
            
            return CrawlStatus.SUCCESS
            
        except Exception as e:
            self.logger.error(f"Error parsing {url}: {str(e)}")
            if settings.DEBUG:
                import traceback
                self.logger.error(traceback.format_exc())
            self.stats['failed'] += 1
            return CrawlStatus.BLOCKED
    
    async def run_auto(
        self,
        count: int = 100
    ) -> Dict[str, Any]:
        """
        è‡ªå‹•çˆ¬å–æœ€æ–°æ–‡ç« 
        
        âœ… FIX #WORK-ORDER-921: ä½¿ç”¨ä¾†æºå°ˆå±¬çš„ concurrent_limit
        âœ… FIX #WORK-ORDER-906: åŠ å…¥è©³ç´°é™¤éŒ¯ Log
        âœ… FIX #WORK-ORDER-902: æ”¯æ´åˆ—è¡¨å¼çˆ¬å– (List-Based Crawling)
        
        ç­–ç•¥ï¼š
        1. å‘¼å« parser.get_latest_id() å–å¾—æœ€æ–° ID
        2. ğŸ” Duck Typing æª¢æŸ¥ï¼šParser æ˜¯å¦æœ‰ get_discovered_ids() æ–¹æ³•
        3. æƒ…å¢ƒ A (åˆ—è¡¨æ¨¡å¼ - MOEA/E-Info)ï¼š
           - å‘¼å« get_discovered_ids() å–å¾— valid_ids
           - å¾ valid_ids ä¸­åˆ‡ç‰‡å–å‰ count å€‹
           - Log: "ğŸ“‹ List-based crawling: using X discovered IDs"
        4. æƒ…å¢ƒ B (æµæ°´è™Ÿæ¨¡å¼ - LTN/UDN/ChinaTimes/CNA)ï¼š
           - ä½¿ç”¨ range(latest_id, latest_id - count, -1)
           - Log: "ğŸ”¢ Range-based crawling: ID X â†’ Y"
        
        Args:
            count: è¦çˆ¬å–çš„æ–‡ç« æ•¸é‡ï¼ˆé è¨­ 100ï¼‰
            
        Returns:
            çˆ¬å–çµæœçµ±è¨ˆ
        """
        self.logger.info(f"ğŸš€ Starting auto crawl: {count} articles")
        
        # æ­¥é©Ÿ 1ï¼šå–å¾—æœ€æ–° ID
        latest_id = await self.parser.get_latest_id()
        if latest_id is None:
            self.logger.error("Failed to get latest ID")
            return {'error': 'Failed to get latest ID'}
        
        self.logger.info(f"   Latest ID: {latest_id:,}")
        
        # âœ… FIX #WORK-ORDER-906: è©³ç´°é™¤éŒ¯ Log
        self.logger.info(f"")
        self.logger.info(f"ğŸ” [Duck Typing Check] Inspecting Parser capabilities...")
        self.logger.info(f"   Parser class: {self.parser.__class__.__name__}")
        self.logger.info(f"   Parser source: {self.parser.source_name}")
        
        # æª¢æŸ¥ hasattr
        has_method = hasattr(self.parser, 'get_discovered_ids')
        self.logger.info(f"   hasattr(parser, 'get_discovered_ids'): {has_method}")
        
        if has_method:
            # æª¢æŸ¥ callable
            is_callable = callable(getattr(self.parser, 'get_discovered_ids'))
            self.logger.info(f"   callable(parser.get_discovered_ids): {is_callable}")
            
            if is_callable:
                # æª¢æŸ¥æ–¹æ³•ç°½å
                method = getattr(self.parser, 'get_discovered_ids')
                self.logger.info(f"   Method object: {method}")
                self.logger.info(f"   Method type: {type(method)}")
        
        # âœ… FIX #WORK-ORDER-902: Duck Typing æª¢æŸ¥
        # æ­¥é©Ÿ 2ï¼šæª¢æŸ¥ Parser æ˜¯å¦æ”¯æ´åˆ—è¡¨å¼çˆ¬å–
        if hasattr(self.parser, 'get_discovered_ids') and callable(getattr(self.parser, 'get_discovered_ids')):
            # ========== æƒ…å¢ƒ Aï¼šåˆ—è¡¨æ¨¡å¼ (MOEA/E-Info) ==========
            self.logger.info(f"")
            self.logger.info(f"âœ… [Duck Typing] Detected list-based parser!")
            self.logger.info(f"   Calling parser.get_discovered_ids()...")
            
            # å–å¾—å·²ç™¼ç¾çš„ ID åˆ—è¡¨
            try:
                valid_ids = self.parser.get_discovered_ids()
                self.logger.info(f"   âœ… get_discovered_ids() returned: {len(valid_ids) if valid_ids else 0} IDs")
                
                if valid_ids:
                    # é¡¯ç¤ºå‰ 10 å€‹ IDï¼ˆé™¤éŒ¯ç”¨ï¼‰
                    preview = valid_ids[:10]
                    self.logger.info(f"   ğŸ“‹ ID preview (first 10): {preview}")
                
            except Exception as e:
                self.logger.error(f"   âŒ get_discovered_ids() failed: {e}")
                import traceback
                self.logger.error(traceback.format_exc())
                valid_ids = None
            
            if not valid_ids:
                self.logger.error("Parser returned empty ID list, falling back to range mode")
                # é™ç´šç‚ºæµæ°´è™Ÿæ¨¡å¼
                start_id = latest_id
                end_id = latest_id - count + 1
                target_ids = list(range(start_id, end_id - 1, -1))
                self.logger.info(f"   ğŸ”¢ Fallback range: ID {start_id:,} â†’ {end_id:,}")
            else:
                # åˆ‡ç‰‡å–å‰ count å€‹
                target_ids = valid_ids[:count]
                
                self.logger.info(f"")
                self.logger.info(f"ğŸ“‹ List-based crawling mode activated!")
                self.logger.info(f"   Using {len(target_ids)} discovered IDs (from {len(valid_ids):,} total)")
                self.logger.info(f"   ID range: {target_ids[0]:,} ... {target_ids[-1]:,}")
            
        else:
            # ========== æƒ…å¢ƒ Bï¼šæµæ°´è™Ÿæ¨¡å¼ (LTN/UDN/ChinaTimes/CNA) ==========
            self.logger.info(f"")
            self.logger.info(f"â„¹ï¸  [Duck Typing] No get_discovered_ids() method found")
            self.logger.info(f"   Falling back to range-based crawling")
            
            # è¨ˆç®—ç¯„åœ
            start_id = latest_id
            end_id = latest_id - count + 1
            
            target_ids = list(range(start_id, end_id - 1, -1))
            
            self.logger.info(f"")
            self.logger.info(f"ğŸ”¢ Range-based crawling mode activated!")
            self.logger.info(f"   ID range: {start_id:,} â†’ {end_id:,}")
        
        # æ­¥é©Ÿ 3ï¼šåŸ·è¡Œçˆ¬å–
        self.logger.info(f"")
        self.logger.info(f"ğŸ¯ Target: {len(target_ids)} articles")
        
        # é‡ç½®çµ±è¨ˆ
        self.stats = {
            'total': len(target_ids),
            'success': 0,
            'failed': 0,
            'skipped': 0,
            'not_found': 0,
            'blocked': 0,
        }
        
        # å‰µå»ºæœƒè©±
        if self.session is None:
            self.session = await self._create_session()
            need_close = True
        else:
            need_close = False
        
        # âœ… FIX #WORK-ORDER-921: ä½¿ç”¨ä¾†æºå°ˆå±¬çš„ concurrent_limit
        semaphore = asyncio.Semaphore(self.concurrent_limit)
        
        async def process_with_semaphore(article_id: int):
            async with semaphore:
                await self._random_delay()
                return await self._process_article(article_id, self.session)
        
        # å‰µå»ºä»»å‹™åˆ—è¡¨
        tasks = [process_with_semaphore(article_id) for article_id in target_ids]
        
        # åŸ·è¡Œæ‰€æœ‰ä»»å‹™
        self.logger.info(f"ğŸ“Š Processing {len(tasks)} articles with {self.concurrent_limit} concurrent requests")
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # è™•ç†çµæœ
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                self.logger.error(f"Task exception for ID {target_ids[i]}: {result}")
                self.stats['blocked'] += 1
        
        # é—œé–‰æœƒè©±
        if need_close:
            await self.close()
        
        # è¼¸å‡ºçµ±è¨ˆ
        self._log_stats()
        
        return self.stats
    
    async def run_range(
        self,
        start_id: int,
        end_id: int,
        reverse: bool = False
    ) -> Dict[str, Any]:
        """
        çˆ¬å–æŒ‡å®šç¯„åœçš„æ–‡ç«  ID
        
        âœ… FIX #WORK-ORDER-921: ä½¿ç”¨ä¾†æºå°ˆå±¬çš„ concurrent_limit
        âœ… FIX #WORK-ORDER-912: åŠ å…¥è·³èºå¤±æ•—é™¤éŒ¯ Log
        âœ… FIX #WORK-ORDER-911: ä¿®æ­£æ™ºèƒ½è·³èºè¨ˆæ•¸é‚è¼¯èˆ‡è¦–è¦ºåŒ–
        âœ… FIX #ENGINE-REFACTOR-V2: å„ªåŒ–è·³èºåˆ¤æ–·é‚è¼¯
        
        Args:
            start_id: èµ·å§‹ ID
            end_id: çµæŸ ID
            reverse: æ˜¯å¦åå‘çˆ¬å–ï¼ˆå¾ start_id éæ¸›åˆ° end_idï¼‰ï¼Œé è¨­ False
            
        Returns:
            çˆ¬å–çµæœçµ±è¨ˆ
        """
        # ç¢ºä¿ start_id <= end_idï¼ˆç•¶ reverse=False æ™‚ï¼‰
        if not reverse and start_id > end_id:
            start_id, end_id = end_id, start_id
        elif reverse and start_id < end_id:
            start_id, end_id = end_id, start_id
        
        # ç”Ÿæˆ ID åˆ—è¡¨
        if reverse:
            direction = "reverse"
            step = -1
        else:
            direction = "forward"
            step = 1
        
        self.logger.info(f"ğŸš€ Starting crawl: ID {start_id:,} â†’ {end_id:,} ({direction})")
        
        # âœ… FIX #WORK-ORDER-911: é¡¯ç¤ºæ™ºèƒ½è·³èºç‹€æ…‹ï¼ˆåŒ…å«å¾ Settings è®€åˆ°çš„æ•¸å€¼ï¼‰
        if self._should_enable_smart_jump():
            self.logger.info(f"   Smart Jump: ENABLED (threshold: {self.SMART_JUMP_THRESHOLD})")
        else:
            self.logger.info(f"   Smart Jump: DISABLED (source: {self.parser.source_name})")
        
        # é‡ç½®çµ±è¨ˆ
        total_range = abs(start_id - end_id) + 1
        self.stats = {
            'total': total_range,
            'success': 0,
            'failed': 0,
            'skipped': 0,
            'not_found': 0,
            'blocked': 0,
        }
        
        # é‡ç½®æ™ºèƒ½è·³èºç‹€æ…‹
        self.consecutive_failures = 0
        self.smart_jump_count = 0
        
        # å‰µå»ºæœƒè©±
        if self.session is None:
            self.session = await self._create_session()
            need_close = True
        else:
            need_close = False
        
        # âœ… FIX #WORK-ORDER-921: ä½¿ç”¨ä¾†æºå°ˆå±¬çš„ concurrent_limit
        semaphore = asyncio.Semaphore(self.concurrent_limit)
        
        # æ”¹ç”¨ while è¿´åœˆæ”¯æ´æ™ºèƒ½è·³èº
        current_id = start_id
        processed_count = 0
        
        async def process_with_semaphore(article_id: int):
            async with semaphore:
                await self._random_delay()
                return await self._process_article(article_id, self.session)
        
        # å‰µå»ºä»»å‹™ä½‡åˆ—
        tasks = []
        task_ids = []
        
        while (not reverse and current_id <= end_id) or (reverse and current_id >= end_id):
            # æ·»åŠ ä»»å‹™åˆ°ä½‡åˆ—
            task = process_with_semaphore(current_id)
            tasks.append(task)
            task_ids.append(current_id)
            processed_count += 1
            
            # æ­£å¸¸éå¢/éæ¸›
            current_id += step
            
            # âœ… FIX #WORK-ORDER-911: æ¯ 10 ç­†å°±è™•ç†ä¸€æ¬¡ï¼ˆåŠ å¿«åæ‡‰é€Ÿåº¦ï¼‰
            if len(tasks) >= 10:
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # âœ… FIX #WORK-ORDER-911: ä¿®æ­£è¨ˆæ•¸é‚è¼¯
                for i, result in enumerate(results):
                    if isinstance(result, Exception):
                        self.logger.error(f"Task exception for ID {task_ids[i]}: {result}")
                        self.stats['blocked'] += 1
                        # âœ… FIX: ç•°å¸¸ä¹Ÿç®—å¤±æ•—ï¼Œé¿å…å¡æ­»
                        self.consecutive_failures += 1
                        continue
                    
                    if result == CrawlStatus.SUCCESS:
                        # âœ… æˆåŠŸï¼šé‡ç½®å¤±æ•—è¨ˆæ•¸
                        self.consecutive_failures = 0
                    
                    elif result == CrawlStatus.NOT_FOUND:
                        # âœ… 404ï¼šå¢åŠ å¤±æ•—è¨ˆæ•¸
                        self.consecutive_failures += 1
                    
                    elif result == CrawlStatus.BLOCKED:
                        # âœ… FIX #WORK-ORDER-911: BLOCKED ä¹Ÿç®—å¤±æ•—
                        # ï¼ˆé‡å°é€£çºŒç„¡æ•ˆ ID å°è‡´çš„ Timeout/Errorï¼‰
                        self.consecutive_failures += 1
                        self.logger.warning(f"ğŸš« Blocked/Error detected.")
                
                # âœ… FIX #WORK-ORDER-911: è¦–è¦ºåŒ–è¨ˆæ•¸å™¨ï¼ˆè®“è€é—†çœ‹åˆ°æ•¸å­—åœ¨è·‘ï¼‰
                if self._should_enable_smart_jump() and self.consecutive_failures > 0:
                    self.logger.info(f"   âš ï¸  Consecutive Failures: {self.consecutive_failures} / {self.SMART_JUMP_THRESHOLD}")
                
                # âœ… æ™ºèƒ½è·³èºæª¢æŸ¥
                if (self.consecutive_failures >= self.SMART_JUMP_THRESHOLD and
                    self._should_enable_smart_jump()):
                    
                    # âœ… FIX #WORK-ORDER-911: ä½¿ç”¨é€™ä¸€æ‰¹æœ€å¾Œä¸€å€‹ ID ä¾†ç®—
                    jump_target = self._calculate_jump_target(task_ids[-1])
                    
                    if jump_target is not None:
                        # æª¢æŸ¥è·³èºç›®æ¨™æ˜¯å¦åœ¨ç¯„åœå…§
                        if (not reverse and jump_target <= end_id) or (reverse and jump_target >= end_id):
                            self.logger.warning(f"")
                            self.logger.warning(f"ğŸš€ [Smart Jump] Triggered! ({self.consecutive_failures} failures)")
                            self.logger.warning(f"   Current ID: {task_ids[-1]:,}")
                            self.logger.warning(f"   Jump target: {jump_target:,}")
                            self.logger.warning(f"   Reason: Consecutive failures threshold reached")
                            
                            current_id = jump_target
                            self.consecutive_failures = 0
                            self.smart_jump_count += 1
                            
                            # æ¸…ç©ºä»»å‹™ä¸¦è·³å‡ºç•¶å‰æ‰¹æ¬¡è™•ç†
                            tasks = []
                            task_ids = []
                            continue
                        else:
                            # âœ… FIX #WORK-ORDER-912: è·³èºç›®æ¨™è¶…å‡ºç¯„åœ
                            self.logger.warning(f"")
                            self.logger.warning(f"âš ï¸  [Smart Jump] Target out of range!")
                            self.logger.warning(f"   Current ID: {task_ids[-1]:,}")
                            self.logger.warning(f"   Jump target: {jump_target:,}")
                            self.logger.warning(f"   End ID: {end_id:,}")
                            self.logger.warning(f"   Reason: Jump target exceeds crawl range, stopping here")
                            # ä¸è·³èºï¼Œç¹¼çºŒçˆ¬å–ç›´åˆ° end_id
                    else:
                        # âœ… FIX #WORK-ORDER-912: æ–°å¢é€™è¡Œï¼šå‘Šè¨´è€é—†ç‚ºä»€éº¼æ²’è·³
                        self.logger.warning(f"")
                        self.logger.warning(f"âš ï¸  [Smart Jump] Condition met but target is None!")
                        self.logger.warning(f"   Current ID: {task_ids[-1]:,}")
                        self.logger.warning(f"   Consecutive failures: {self.consecutive_failures}")
                        self.logger.warning(f"   Reason: Failed to parse date from ID or calculate jump target")
                        self.logger.warning(f"   Action: Continuing normal crawl (no jump)")
                
                # æ¸…ç©ºä»»å‹™ä½‡åˆ—
                tasks = []
                task_ids = []
        
        # è™•ç†å‰©é¤˜ä»»å‹™
        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # åŒæ¨£çš„ç‹€æ…‹è™•ç†é‚è¼¯
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    self.logger.error(f"Task exception for ID {task_ids[i]}: {result}")
                    self.stats['blocked'] += 1
                    self.consecutive_failures += 1
                    continue
                
                if result == CrawlStatus.SUCCESS:
                    self.consecutive_failures = 0
                elif result == CrawlStatus.NOT_FOUND:
                    self.consecutive_failures += 1
                elif result == CrawlStatus.BLOCKED:
                    self.consecutive_failures += 1
                    self.logger.warning(f"ğŸš« Blocked/Error detected.")
        
        # âœ… FIX #ENGINE-CLOSE-TIMEOUT: é—œé–‰æœƒè©±ï¼ˆåŠ ä¸Š timeout ä¿è­·ï¼‰
        if need_close:
            await self.close()
        
        # è¼¸å‡ºçµ±è¨ˆ
        self._log_stats()
        
        # è¼¸å‡ºæ™ºèƒ½è·³èºçµ±è¨ˆ
        if self.smart_jump_count > 0:
            self.logger.info(f"")
            self.logger.info(f"ğŸš€ Smart Jump Statistics:")
            self.logger.info(f"   Total jumps: {self.smart_jump_count}")
            self.logger.info(f"   Estimated time saved: ~{self.smart_jump_count * self.SMART_JUMP_THRESHOLD * 0.5:.1f}s")
        
        return self.stats
    
    def _log_stats(self) -> None:
        """
        è¼¸å‡ºçˆ¬å–çµ±è¨ˆè³‡è¨Š
        
        âœ… FIX #ENGINE-REFACTOR-V2: æ–°å¢ BLOCKED çµ±è¨ˆ
        """
        self.logger.info("=" * 60)
        self.logger.info("ğŸ“Š Crawl Statistics:")
        self.logger.info(f"   Total:     {self.stats['total']:,}")
        self.logger.info(f"   Success:   {self.stats['success']:,} âœ…")
        self.logger.info(f"   Failed:    {self.stats['failed']:,} âŒ")
        self.logger.info(f"   Skipped:   {self.stats['skipped']:,} â­ï¸")
        self.logger.info(f"   Not Found: {self.stats['not_found']:,} ğŸ”")
        self.logger.info(f"   Blocked:   {self.stats['blocked']:,} ğŸš«")
        
        if self.stats['total'] > 0:
            success_rate = (self.stats['success'] / self.stats['total']) * 100
            self.logger.info(f"   Success Rate: {success_rate:.2f}%")
        
        self.logger.info("=" * 60)
    
    async def close(self):
        """
        é—œé–‰ Engine ä¸¦æ¸…ç†è³‡æº
        
        âœ… FIX #WORK-ORDER-902: ç§»é™¤ Pipeline.close() å‘¼å«
        âœ… FIX #ENGINE-CLOSE-TIMEOUT: åŠ ä¸Š timeout ä¿è­·ï¼Œé¿å… curl_cffi å¡ä½
        """
        try:
            if self.session is not None:
                # âœ… åŠ ä¸Š timeout ä¿è­·
                await asyncio.wait_for(
                    self.session.close(),
                    timeout=5.0
                )
        except asyncio.TimeoutError:
            self.logger.warning("âš ï¸  Session close timeout, forcing shutdown")
        except Exception as e:
            self.logger.error(f"âŒ Error closing session: {e}")
        finally:
            self.session = None
            self.logger.info("âœ… Engine closed")
        
        # âœ… FIX #WORK-ORDER-902: ç§»é™¤ Pipeline.close() å‘¼å«
        # åŸå› ï¼šPipeline é¡åˆ¥æ²’æœ‰å®šç¾© close() æ–¹æ³•
        # TSVWriter åœ¨å¯«å…¥å¾Œæœƒè‡ªå‹•é—œé–‰æª”æ¡ˆï¼Œä¸éœ€è¦é¡¯å¼é—œé–‰
    
    async def run_list(
        self,
        url_list: List[str]
    ) -> Dict[str, Any]:
        """
        çˆ¬å–æŒ‡å®šçš„ URL åˆ—è¡¨
        
        âœ… FIX #WORK-ORDER-921: ä½¿ç”¨ä¾†æºå°ˆå±¬çš„ concurrent_limit
        
        Args:
            url_list: URL åˆ—è¡¨
            
        Returns:
            çˆ¬å–çµæœçµ±è¨ˆ
        """
        self.logger.info(f"ğŸš€ Starting crawl: {len(url_list)} URLs")
        
        # é‡ç½®çµ±è¨ˆ
        self.stats = {
            'total': len(url_list),
            'success': 0,
            'failed': 0,
            'skipped': 0,
            'not_found': 0,
            'blocked': 0,
        }
        
        # å‰µå»ºæœƒè©±
        if self.session is None:
            self.session = await self._create_session()
            need_close = True
        else:
            need_close = False
        
        # âœ… FIX #WORK-ORDER-921: ä½¿ç”¨ä¾†æºå°ˆå±¬çš„ concurrent_limit
        semaphore = asyncio.Semaphore(self.concurrent_limit)
        
        async def process_url(url: str):
            async with semaphore:
                await self._random_delay()
                
                # æª¢æŸ¥æ˜¯å¦å·²çˆ¬å–
                if self._is_crawled(url):
                    self.logger.debug(f"â­ï¸  Skipping already crawled: {url}")
                    self.stats['skipped'] += 1
                    return CrawlStatus.SUCCESS
                
                # ç²å– HTML
                html, status = await self._fetch(url, self.session)
                
                if status == CrawlStatus.NOT_FOUND:
                    self.stats['not_found'] += 1
                    return status
                
                elif status == CrawlStatus.BLOCKED:
                    self.stats['blocked'] += 1
                    return status
                
                if html is None:
                    self.stats['failed'] += 1
                    return CrawlStatus.BLOCKED
                
                # è§£æ HTML
                try:
                    data = await self.parser.parse(html, url)
                    if data is None:
                        self.stats['failed'] += 1
                        return CrawlStatus.NOT_FOUND
                    
                    # æ¨™è¨˜ç‚ºå·²çˆ¬å–
                    self._mark_as_crawled(url)
                    
                    # è‡ªå‹•å„²å­˜
                    if self.auto_save:
                        success = await self.pipeline.process_and_save(url, data)
                        if success:
                            self.logger.info(f"âœ… Saved: {url}")
                            self.stats['success'] += 1
                        else:
                            self.logger.error(f"âŒ Failed to save: {url}")
                            self.stats['failed'] += 1
                    else:
                        self.stats['success'] += 1
                    
                    return CrawlStatus.SUCCESS
                    
                except Exception as e:
                    self.logger.error(f"Error parsing {url}: {str(e)}")
                    if settings.DEBUG:
                        import traceback
                        self.logger.error(traceback.format_exc())
                    self.stats['failed'] += 1
                    return CrawlStatus.BLOCKED
        
        # å‰µå»ºä»»å‹™åˆ—è¡¨
        tasks = [process_url(url) for url in url_list]
        
        # åŸ·è¡Œæ‰€æœ‰ä»»å‹™
        self.logger.info(f"ğŸ“Š Processing {len(tasks)} URLs with {self.concurrent_limit} concurrent requests")
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # âœ… FIX #ENGINE-CLOSE-TIMEOUT: é—œé–‰æœƒè©±ï¼ˆåŠ ä¸Š timeout ä¿è­·ï¼‰
        if need_close:
            await self.close()
        
        # è¼¸å‡ºçµ±è¨ˆ
        self._log_stats()
        
        return self.stats
