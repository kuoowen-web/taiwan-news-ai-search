"""
main.py - NLWeb v2 ä¸»ç¨‹å¼å…¥å£

é€šç”¨æ–°èçˆ¬èŸ²ç³»çµ±ï¼Œæ”¯æ´å¤šç¨®æ–°èä¾†æºã€‚

åŸ·è¡Œæ¨¡å¼ï¼š
1. ç¯„åœæ¨¡å¼ï¼šæŒ‡å®š ID ç¯„åœçˆ¬å–
2. è‡ªå‹•æ¨¡å¼ï¼šè‡ªå‹•ç²å–æœ€æ–° ID ä¸¦å‘å‰çˆ¬å–
3. æ—¥æœŸæ¨¡å¼ï¼šæ ¹æ“šæ—¥æœŸç¯„åœè‡ªå‹•å®šä½ IDï¼ˆéœ€é…åˆ Navigatorï¼‰

âœ… FIX #WORK-ORDER-905: å®Œå…¨é€šç”¨åŒ–ï¼Œç§»é™¤æ‰€æœ‰ä¾†æºç‰¹å®šé‚è¼¯

è¨­è¨ˆåŸå‰‡ï¼š
- å®Œå…¨é€šç”¨ï¼Œä¸åŒ…å«ä»»ä½•ç¶²ç«™ç‰¹å®šé‚è¼¯
- é€éå·¥å» æ¨¡å¼å‹•æ…‹è¼‰å…¥ Parser
- æ‰€æœ‰é…ç½®é€éå‘½ä»¤åˆ—åƒæ•¸æˆ–é…ç½®æª”
"""

import asyncio
import argparse
import logging
import sys
from datetime import datetime
from typing import Optional

from config import settings
from src.parsers.factory import CrawlerFactory, list_available_sources
from src.core.engine import CrawlerEngine
from src.core.navigator import DateNavigator


def setup_logging(verbose: bool = False) -> None:
    """
    è¨­ç½®æ—¥èªŒç³»çµ±
    
    Args:
        verbose: æ˜¯å¦é¡¯ç¤ºè©³ç´°æ—¥èªŒ
    """
    level = logging.DEBUG if verbose else logging.INFO
    
    logging.basicConfig(
        level=level,
        format=settings.LOG_FORMAT,
        datefmt=settings.LOG_DATE_FORMAT,
        handlers=[
            logging.StreamHandler(sys.stdout)
        ]
    )


def create_parser() -> argparse.ArgumentParser:
    """
    å‰µå»ºå‘½ä»¤åˆ—åƒæ•¸è§£æå™¨
    
    Returns:
        ArgumentParser å¯¦ä¾‹
    """
    parser = argparse.ArgumentParser(
        description='NLWeb v2 - Universal News Crawler',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # çˆ¬å–æŒ‡å®šç¯„åœï¼ˆå¾æ–°åˆ°èˆŠï¼‰
  python main.py --source ltn --id-start 4567890 --id-end 4567800
  
  # è‡ªå‹•ç²å–æœ€æ–° ID ä¸¦å‘å‰çˆ¬å– 100 ç¯‡
  python main.py --source ltn --auto-latest --count 100
  
  # æ ¹æ“šæ—¥æœŸç¯„åœçˆ¬å–
  python main.py --source ltn --date-start 2025-12-01 --date-end 2025-12-07
  
  # ä¹¾è·‘æ¨¡å¼ï¼ˆå®Œæ•´æ¨¡æ“¬çˆ¬å–æµç¨‹ä½†ä¸å„²å­˜ï¼‰
  python main.py --source ltn --id-start 4567890 --id-end 4567800 --dry-run
  
  # åˆ—å‡ºæ‰€æœ‰æ”¯æ´çš„æ–°èä¾†æº
  python main.py --list-sources
        """
    )
    
    # åŸºæœ¬åƒæ•¸
    parser.add_argument(
        '--source',
        type=str,
        choices=list_available_sources(),
        help='News source to crawl (e.g., ltn, udn, moea)'
    )
    
    parser.add_argument(
        '--list-sources',
        action='store_true',
        help='List all available news sources and exit'
    )
    
    # ID ç¯„åœæ¨¡å¼
    id_group = parser.add_argument_group('ID Range Mode')
    id_group.add_argument(
        '--id-start',
        type=int,
        help='Starting article ID'
    )
    id_group.add_argument(
        '--id-end',
        type=int,
        help='Ending article ID'
    )
    id_group.add_argument(
        '--reverse',
        action='store_true',
        default=True,
        help='Crawl from start to end in reverse order (default: True)'
    )
    
    # è‡ªå‹•æ¨¡å¼
    auto_group = parser.add_argument_group('Auto Mode')
    auto_group.add_argument(
        '--auto-latest',
        action='store_true',
        help='Automatically fetch the latest article ID'
    )
    auto_group.add_argument(
        '--count',
        type=int,
        default=100,
        help='Number of articles to crawl in auto mode (default: 100)'
    )
    
    # æ—¥æœŸæ¨¡å¼
    date_group = parser.add_argument_group('Date Range Mode')
    date_group.add_argument(
        '--date-start',
        type=str,
        help='Start date (YYYY-MM-DD)'
    )
    date_group.add_argument(
        '--date-end',
        type=str,
        help='End date (YYYY-MM-DD)'
    )
    date_group.add_argument(
        '--search-min-id',
        type=int,
        help='Minimum ID for date search (optional)'
    )
    date_group.add_argument(
        '--search-max-id',
        type=int,
        help='Maximum ID for date search (optional)'
    )
    
    # Parser åƒæ•¸ï¼ˆé€šç”¨ï¼‰
    parser_group = parser.add_argument_group('Parser Options')
    parser_group.add_argument(
        '--max-pages',
        type=int,
        help='Maximum pages to crawl (for list-based parsers like MOEA)'
    )
    
    # å…¶ä»–é¸é …
    parser.add_argument(
        '--no-auto-save',
        action='store_true',
        help='Disable automatic saving (for testing)'
    )
    parser.add_argument(
        '--verbose',
        '-v',
        action='store_true',
        help='Enable verbose logging'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='Dry run mode: simulate full crawling process without saving data'
    )
    
    return parser


async def run_id_range_mode(
    engine: CrawlerEngine,
    start_id: int,
    end_id: int,
    reverse: bool = True
) -> dict:
    """
    åŸ·è¡Œ ID ç¯„åœæ¨¡å¼
    
    Args:
        engine: CrawlerEngine å¯¦ä¾‹
        start_id: èµ·å§‹ ID
        end_id: çµæŸ ID
        reverse: æ˜¯å¦åå‘çˆ¬å–
        
    Returns:
        çˆ¬å–çµ±è¨ˆçµæœ
    """
    logger = logging.getLogger('main')
    logger.info(f"ğŸš€ Starting ID Range Mode")
    logger.info(f"   Range: {start_id} â†’ {end_id}")
    logger.info(f"   Direction: {'Reverse (new to old)' if reverse else 'Forward (old to new)'}")
    
    stats = await engine.run_range(start_id, end_id, reverse=reverse)
    return stats


async def run_auto_mode(
    engine: CrawlerEngine,
    count: int = 100
) -> dict:
    """
    åŸ·è¡Œè‡ªå‹•æ¨¡å¼ï¼ˆç²å–æœ€æ–° ID ä¸¦å‘å‰çˆ¬å–ï¼‰
    
    âœ… FIX #WORK-ORDER-905: ä½¿ç”¨ Engine çš„ run_auto() æ–¹æ³•
    
    Args:
        engine: CrawlerEngine å¯¦ä¾‹
        count: è¦çˆ¬å–çš„æ–‡ç« æ•¸é‡
        
    Returns:
        çˆ¬å–çµ±è¨ˆçµæœ
    """
    logger = logging.getLogger('main')
    logger.info(f"ğŸš€ Starting Auto Mode")
    logger.info(f"   Target: {count} articles")
    
    # âœ… ä½¿ç”¨ Engine çš„ run_auto() æ–¹æ³•ï¼ˆè‡ªå‹•é©é…åˆ—è¡¨å¼/æµæ°´è™Ÿå¼ï¼‰
    stats = await engine.run_auto(count=count)
    return stats


async def run_date_range_mode(
    engine: CrawlerEngine,
    parser,
    start_date: datetime,
    end_date: datetime,
    search_min_id: Optional[int] = None,
    search_max_id: Optional[int] = None
) -> dict:
    """
    åŸ·è¡Œæ—¥æœŸç¯„åœæ¨¡å¼ï¼ˆä½¿ç”¨ Navigator å®šä½ IDï¼‰
    
    Args:
        engine: CrawlerEngine å¯¦ä¾‹
        parser: Parser å¯¦ä¾‹
        start_date: é–‹å§‹æ—¥æœŸ
        end_date: çµæŸæ—¥æœŸ
        search_min_id: æœå°‹ç¯„åœæœ€å° IDï¼ˆå¯é¸ï¼‰
        search_max_id: æœå°‹ç¯„åœæœ€å¤§ IDï¼ˆå¯é¸ï¼‰
        
    Returns:
        çˆ¬å–çµ±è¨ˆçµæœ
    """
    logger = logging.getLogger('main')
    logger.info(f"ğŸš€ Starting Date Range Mode")
    logger.info(f"   Date Range: {start_date.strftime('%Y-%m-%d')} â†’ {end_date.strftime('%Y-%m-%d')}")
    
    # å‰µå»º Navigator
    navigator = DateNavigator(parser.get_date, parser.source_name)
    
    # å¦‚æœæœªæä¾›æœå°‹ç¯„åœï¼Œå˜—è©¦è‡ªå‹•ç²å–
    if search_max_id is None:
        logger.info("   Fetching latest article ID for search range...")
        
        search_max_id = await parser.get_latest_id()
        
        if search_max_id is None:
            logger.error("âŒ Failed to determine search range")
            return {'error': 'Failed to determine search range'}
        
        logger.info(f"   Latest ID: {search_max_id}")
    
    if search_min_id is None:
        # é è¨­æœå°‹ç¯„åœï¼šæœ€æ–° ID å¾€å‰æ¨ 100 è¬
        search_min_id = max(1, search_max_id - 1000000)
    
    logger.info(f"   Search Range: [{search_min_id}, {search_max_id}]")
    logger.info(f"   Locating article IDs for date range...")
    
    # ä½¿ç”¨ Navigator å®šä½ ID ç¯„åœ
    id_range = await navigator.find_date_range(
        start_date=start_date,
        end_date=end_date,
        min_id=search_min_id,
        max_id=search_max_id
    )
    
    if id_range is None:
        logger.error("âŒ Failed to locate article IDs for date range")
        return {'error': 'Failed to locate IDs'}
    
    start_id, end_id = id_range
    logger.info(f"   Located ID Range: [{start_id}, {end_id}]")
    
    # åŸ·è¡Œçˆ¬å–
    stats = await engine.run_range(start_id, end_id, reverse=False)
    return stats


async def main_async(args: argparse.Namespace) -> int:
    """
    éåŒæ­¥ä¸»å‡½æ•¸
    
    âœ… FIX #WORK-ORDER-905: å®Œå…¨é€šç”¨åŒ–ï¼Œç§»é™¤æ‰€æœ‰ä¾†æºç‰¹å®šé‚è¼¯
    
    Args:
        args: å‘½ä»¤åˆ—åƒæ•¸
        
    Returns:
        é€€å‡ºç¢¼ï¼ˆ0 è¡¨ç¤ºæˆåŠŸï¼‰
    """
    logger = logging.getLogger('main')
    
    # åˆ—å‡ºä¾†æºæ¨¡å¼
    if args.list_sources:
        sources = list_available_sources()
        print("Available news sources:")
        for source in sources:
            print(f"  - {source}")
        return 0
    
    # æª¢æŸ¥å¿…è¦åƒæ•¸
    if not args.source:
        logger.error("âŒ --source is required (use --list-sources to see available sources)")
        return 1
    
    # Dry run æ¨¡å¼æç¤º
    if args.dry_run:
        logger.info("ğŸ” Dry run mode enabled")
        logger.info("   Will simulate full crawling process")
        logger.info("   No data will be saved to disk")
    
    try:
        # âœ… FIX #WORK-ORDER-905: æº–å‚™é€šç”¨åƒæ•¸
        parser_kwargs = {}
        
        # å‚³é countï¼ˆç”¨æ–¼è¨ˆç®— max_pagesï¼‰
        if args.count:
            parser_kwargs['count'] = args.count
        
        # å‚³é max_pagesï¼ˆå„ªå…ˆç´šé«˜æ–¼ countï¼‰
        if args.max_pages:
            parser_kwargs['max_pages'] = args.max_pages
        
        # å‚³é target_dateï¼ˆæœªä¾†æ”¯æ´æ—¥æœŸéæ¿¾ï¼‰
        if args.date_start:
            try:
                target_date = datetime.strptime(args.date_start, '%Y-%m-%d')
                parser_kwargs['target_date'] = target_date
            except ValueError:
                pass
        
        # âœ… FIX #WORK-ORDER-905: é€šç”¨å‘¼å«ï¼ˆä¸ç®¡ä½ æ˜¯ moea, ltn é‚„æ˜¯ future_sourceï¼‰
        logger.info(f"ğŸ“¦ Loading parser for source: {args.source}")
        if parser_kwargs:
            logger.info(f"   Parser kwargs: {parser_kwargs}")
        
        parser = CrawlerFactory.get_parser(args.source, **parser_kwargs)
        
        if parser is None:
            logger.error(f"âŒ Failed to load parser for source: {args.source}")
            return 1
        
        logger.info(f"âœ… Parser loaded: {parser.__class__.__name__}")
        
    except Exception as e:
        logger.error(f"âŒ Failed to load parser: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1
    
    try:
        # å‰µå»ºå¼•æ“æ™‚è€ƒæ…® dry_run æ¨¡å¼
        auto_save = (not args.no_auto_save) and (not args.dry_run)
        engine = CrawlerEngine(parser, auto_save=auto_save)
        logger.info(f"ğŸ”§ Engine initialized (auto_save={auto_save})")
        
        # åˆ¤æ–·åŸ·è¡Œæ¨¡å¼
        stats = None
        
        # æ¨¡å¼ 1ï¼šæ—¥æœŸç¯„åœæ¨¡å¼
        if args.date_start and args.date_end:
            try:
                start_date = datetime.strptime(args.date_start, '%Y-%m-%d')
                end_date = datetime.strptime(args.date_end, '%Y-%m-%d')
                
                stats = await run_date_range_mode(
                    engine=engine,
                    parser=parser,
                    start_date=start_date,
                    end_date=end_date,
                    search_min_id=args.search_min_id,
                    search_max_id=args.search_max_id
                )
            except ValueError as e:
                logger.error(f"âŒ Invalid date format: {e}")
                return 1
        
        # æ¨¡å¼ 2ï¼šè‡ªå‹•æ¨¡å¼
        elif args.auto_latest:
            stats = await run_auto_mode(
                engine=engine,
                count=args.count
            )
        
        # æ¨¡å¼ 3ï¼šID ç¯„åœæ¨¡å¼
        elif args.id_start and args.id_end:
            stats = await run_id_range_mode(
                engine=engine,
                start_id=args.id_start,
                end_id=args.id_end,
                reverse=args.reverse
            )
        
        else:
            logger.error("âŒ Must specify one of: --id-start/--id-end, --auto-latest, or --date-start/--date-end")
            return 1
        
        # æª¢æŸ¥çµæœ
        if stats and 'error' in stats:
            logger.error(f"âŒ Crawl failed: {stats['error']}")
            return 1
        
        # è¼¸å‡ºæœ€çµ‚çµ±è¨ˆ
        if stats:
            logger.info("")
            logger.info("=" * 60)
            
            # Dry run æ¨¡å¼çš„ç‰¹æ®Šæç¤º
            if args.dry_run:
                logger.info("ğŸ” Dry Run Completed Successfully!")
                logger.info("   (No data was saved to disk)")
            else:
                logger.info("ğŸ‰ Crawl Completed Successfully!")
            
            logger.info(f"   Total:     {stats['total']}")
            logger.info(f"   Success:   {stats['success']} âœ…")
            logger.info(f"   Failed:    {stats['failed']} âŒ")
            logger.info(f"   Skipped:   {stats['skipped']} â­ï¸")
            logger.info(f"   Not Found: {stats['not_found']} ğŸ”")
            
            if stats['total'] > 0:
                success_rate = (stats['success'] / stats['total']) * 100
                logger.info(f"   Success Rate: {success_rate:.2f}%")
            
            logger.info("=" * 60)
        
        return 0
        
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸  Interrupted by user")
        return 130
    except Exception as e:
        logger.error(f"âŒ Unexpected error: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


def main() -> int:
    """
    ä¸»å‡½æ•¸å…¥å£
    
    Returns:
        é€€å‡ºç¢¼
    """
    # è§£æå‘½ä»¤åˆ—åƒæ•¸
    parser = create_parser()
    args = parser.parse_args()
    
    # è¨­ç½®æ—¥èªŒ
    setup_logging(verbose=args.verbose)
    
    # åŸ·è¡ŒéåŒæ­¥ä¸»å‡½æ•¸
    try:
        return asyncio.run(main_async(args))
    except KeyboardInterrupt:
        print("\nâš ï¸  Interrupted by user")
        return 130


if __name__ == '__main__':
    sys.exit(main())
